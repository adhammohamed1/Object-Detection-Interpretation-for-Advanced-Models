{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Faster R-CNN: Explained](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Faster R-CNN: Explained](#toc1_)    \n",
    "  - [Faster R-CNN Architecture](#toc1_1_)    \n",
    "  - [Incorporating the Feature Pyramid Network (FPN)](#toc1_2_)    \n",
    "  - [Utility Functions](#toc1_3_)    \n",
    "  - [Inference & Evaluation (IoU & mAP)](#toc1_4_)    \n",
    "    - [Inference](#toc1_4_1_)    \n",
    "    - [Computing IoU](#toc1_4_2_)    \n",
    "  - [Activation Visualization](#toc1_5_)    \n",
    "  - [GradCAM](#toc1_6_)    \n",
    "  - [GradCAM++](#toc1_7_)    \n",
    "  - [EigenCAM](#toc1_8_)    \n",
    "  - [AblationCAM](#toc1_9_)    \n",
    "  - [Deep Feature Factorizations.](#toc1_10_)    \n",
    "  - [ScoreCAM](#toc1_11_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Faster R-CNN Architecture](#toc0_)\n",
    "\n",
    "Faster R-CNN is a popular object detection algorithm introduced by Shaoqing Ren et al. in their 2015 paper, [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/abs/1506.01497). Faster R-CNN is an extension of the [R-CNN](https://www.kaggle.com/achrafkhazri/r-cnn-explained) and [Fast R-CNN](https://www.kaggle.com/achrafkhazri/fast-r-cnn-explained) algorithms, which were also introduced by the same authors. Faster R-CNN is an end-to-end object detection algorithm that uses a convolutional neural network (CNN) to extract features from an input image, a region proposal network (RPN) to generate object proposals, and a series of convolutional layers to classify and refine the object proposals. The following figure shows the architecture of the Faster R-CNN algorithm:\n",
    "\n",
    "1. **Backbone Network**: The backbone network is responsible for extracting high-level features from the input image. It typically consists of a convolutional neural network (CNN) architecture, such as ResNet or MobileNet, which is pre-trained on a large-scale image classification dataset like ImageNet. The backbone network transforms the input image into a feature map that encodes the spatial information of the image.\n",
    "\n",
    "2. **Region Proposal Network (RPN)**: The RPN is a fully convolutional network that generates a set of object proposals, which are potential bounding box regions that may contain objects of interest. The RPN takes the feature map from the backbone network as input and applies a set of anchor boxes at each spatial location. For each anchor box, the RPN predicts the probability of it containing an object and the corresponding bounding box coordinates. The RPN uses a binary classification loss and a bounding box regression loss to train the network.\n",
    "\n",
    "3. **Region-based Convolutional Neural Network (R-CNN)**: The R-CNN takes the object proposals generated by the RPN and performs object classification and bounding box regression. It consists of a series of fully connected layers and softmax classifiers to classify the object proposals into different classes. The R-CNN also refines the bounding box coordinates predicted by the RPN to improve the localization accuracy. The R-CNN is trained using a multi-task loss that combines the classification loss and the bounding box regression loss.\n",
    "\n",
    "The Faster R-CNN algorithm combines these three components to achieve accurate and efficient object detection. By using the RPN to generate object proposals, Faster R-CNN eliminates the need for manually defining anchor boxes and improves the efficiency of the detection process. The backbone network provides the necessary feature representation for object detection, while the R-CNN performs the final classification and localization. Overall, Faster R-CNN has become a widely used algorithm in the field of computer vision and has achieved state-of-the-art performance on various object detection benchmarks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Incorporating the Feature Pyramid Network (FPN)](#toc0_)\n",
    "The Feature Pyramid Network (FPN) can be incorporated into the Faster R-CNN architecture to improve the detection performance, especially for objects at different scales. FPN addresses the challenge of detecting objects at both small and large scales by creating a feature pyramid that captures multi-scale information.\n",
    "\n",
    "In the Faster R-CNN architecture, the backbone network extracts high-level features from the input image. However, these features are typically at a single scale, which may not be sufficient for detecting objects of different sizes. This is because objects can vary in scale within an image, and using features from a single scale may result in missed detections or inaccurate bounding box predictions.\n",
    "\n",
    "To address this issue, FPN introduces a top-down pathway and lateral connections to create a feature pyramid. The top-down pathway upsamples the features from higher resolution levels to lower resolution levels, while the lateral connections merge the upsampled features with the features at each level of the pyramid. This process creates a set of feature maps at different scales, where each level of the pyramid captures information at a specific scale.\n",
    "\n",
    "By incorporating FPN into the Faster R-CNN architecture, we can use the feature pyramid to generate object proposals and perform object classification and bounding box regression at multiple scales. This allows the model to effectively detect objects of different sizes and improves the overall detection accuracy.\n",
    "\n",
    "For example, when detecting small objects, the FPN can leverage the high-resolution features from the top of the pyramid, which contain fine-grained details. On the other hand, when detecting large objects, the FPN can utilize the low-resolution features from the bottom of the pyramid, which capture the global context of the image. By combining features from multiple scales, the model becomes more robust and capable of detecting objects across a wide range of sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Utility Functions](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "from pytorch_grad_cam.ablation_layer import AblationLayerFasterRCNN\n",
    "from pytorch_grad_cam import GradCAM, AblationCAM, EigenCAM, ScoreCAM, GradCAMPlusPlus, DeepFeatureFactorization\n",
    "from pytorch_grad_cam.utils.model_targets import FasterRCNNBoxScoreTarget\n",
    "from pytorch_grad_cam.utils.reshape_transforms import fasterrcnn_reshape_transform\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, scale_cam_image, show_factorization_on_image\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "# Defining COCO categories\n",
    "coco_labels = ['__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
    "              'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', \n",
    "              'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', \n",
    "              'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella',\n",
    "              'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n",
    "              'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
    "              'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass', 'cup', 'fork',\n",
    "              'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n",
    "              'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "              'potted plant', 'bed', 'N/A', 'dining table', 'N/A', 'N/A', 'toilet',\n",
    "              'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n",
    "              'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book', 'clock', 'vase',\n",
    "              'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
    "              \n",
    "def read_image(dataset_dir, image_name) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Read and preprocess an image from the dataset directory.\n",
    "\n",
    "    Args:\n",
    "        dataset_dir (str): The directory path of the dataset.\n",
    "        image_name (str): The name of the image file.\n",
    "\n",
    "    Returns:\n",
    "        image (numpy.ndarray): The preprocessed image as a NumPy array.\n",
    "\n",
    "    \"\"\"\n",
    "    # Read the image using OpenCV\n",
    "    image = cv2.imread(dataset_dir + image_name)\n",
    "\n",
    "    # Convert the image from BGR to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "\n",
    "def predict(input_tensor, model, detection_threshold) -> tuple:\n",
    "    \"\"\"\n",
    "    Perform object detection on an input tensor using a Faster R-CNN model.\n",
    "\n",
    "    Args:\n",
    "        input_tensor (torch.Tensor): The input tensor to perform object detection on.\n",
    "        model (torch.nn.Module): The Faster R-CNN model.\n",
    "        detection_threshold (float): The minimum confidence score threshold for object detection.\n",
    "\n",
    "    Returns:\n",
    "        boxes (numpy.ndarray): An array of bounding boxes for the detected objects.\n",
    "        classes (list): A list of class labels for the detected objects.\n",
    "        labels (numpy.ndarray): An array of class labels for the detected objects.\n",
    "        indices (list): A list of indices corresponding to the detected objects.\n",
    "    \"\"\"\n",
    "    # Perform object detection using the model\n",
    "    outputs = model(input_tensor)\n",
    "\n",
    "    # Extract the predicted classes, labels, scores, and bounding boxes\n",
    "    pred_classes = [coco_labels[i] for i in outputs[0]['labels'].cpu().numpy()]\n",
    "    pred_labels = outputs[0]['labels'].cpu().numpy()\n",
    "    pred_scores = outputs[0]['scores'].detach().cpu().numpy()\n",
    "    pred_bboxes = outputs[0]['boxes'].detach().cpu().numpy()\n",
    "\n",
    "    # Initialize empty lists for storing the filtered results\n",
    "    boxes, classes, labels, indices = [], [], [], []\n",
    "\n",
    "    # Filter the results based on the detection threshold\n",
    "    for index in range(len(pred_scores)):\n",
    "        if pred_scores[index] >= detection_threshold:\n",
    "            boxes.append(pred_bboxes[index].astype(np.int32))\n",
    "            classes.append(pred_classes[index])\n",
    "            labels.append(pred_labels[index])\n",
    "            indices.append(index)\n",
    "\n",
    "    # Convert the boxes to numpy array\n",
    "    boxes = np.int32(boxes)\n",
    "\n",
    "    return boxes, classes, labels, indices\n",
    "\n",
    "\n",
    "def predict_with_scores(input_tensor, model, detection_threshold) -> tuple:\n",
    "    \"\"\"\n",
    "    Perform object detection on an input tensor using a Faster R-CNN model and return the bounding boxes,\n",
    "    class labels, labels, indices, and scores for the detected objects.\n",
    "\n",
    "    Args:\n",
    "        input_tensor (torch.Tensor): The input tensor to perform object detection on.\n",
    "        model (torch.nn.Module): The Faster R-CNN model.\n",
    "        detection_threshold (float): The minimum confidence score threshold for object detection.\n",
    "\n",
    "    Returns:\n",
    "        boxes (numpy.ndarray): An array of bounding boxes for the detected objects.\n",
    "        classes (list): A list of class labels for the detected objects.\n",
    "        labels (numpy.ndarray): An array of class labels for the detected objects.\n",
    "        indices (list): A list of indices corresponding to the detected objects.\n",
    "        scores (numpy.ndarray): An array of confidence scores for the detected objects.\n",
    "    \"\"\"\n",
    "    # Perform object detection using the model\n",
    "    outputs = model(input_tensor)\n",
    "\n",
    "    # Extract the predicted classes, labels, scores, and bounding boxes\n",
    "    pred_classes = [coco_labels[i] for i in outputs[0]['labels'].cpu().numpy()]\n",
    "    pred_labels = outputs[0]['labels'].cpu().numpy()\n",
    "    pred_scores = outputs[0]['scores'].detach().cpu().numpy()\n",
    "    pred_bboxes = outputs[0]['boxes'].detach().cpu().numpy()\n",
    "\n",
    "    # Initialize empty lists for storing the filtered results\n",
    "    boxes, classes, labels, indices, scores = [], [], [], [], []\n",
    "\n",
    "    # Filter the results based on the detection threshold\n",
    "    for index in range(len(pred_scores)):\n",
    "        if pred_scores[index] >= detection_threshold:\n",
    "            boxes.append(pred_bboxes[index].astype(np.int32))\n",
    "            classes.append(pred_classes[index])\n",
    "            labels.append(pred_labels[index])\n",
    "            indices.append(index)\n",
    "            scores.append(pred_scores[index])\n",
    "\n",
    "    # Convert the boxes to numpy array\n",
    "    boxes = np.int32(boxes)\n",
    "\n",
    "    return boxes, classes, labels, indices, scores\n",
    "\n",
    "\n",
    "def draw_boxes(boxes, labels, classes, image) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Draw bounding boxes and class labels on the input image.\n",
    "\n",
    "    Args:\n",
    "        boxes (numpy.ndarray): An array of bounding boxes for the detected objects.\n",
    "        labels (numpy.ndarray): An array of class labels for the detected objects.\n",
    "        classes (list): A list of class labels for the detected objects.\n",
    "        image (numpy.ndarray): The input image.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The image with bounding boxes and class labels drawn on it.\n",
    "    \"\"\"\n",
    "    for i, box in enumerate(boxes):\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (int(box[0]), int(box[1])),\n",
    "            (int(box[2]), int(box[3])),\n",
    "            (0, 255, 0), 2\n",
    "        )\n",
    "        cv2.putText(image, classes[i], (int(box[0]), int(box[1] - 5)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2,\n",
    "                    lineType=cv2.LINE_AA)\n",
    "    return image\n",
    "\n",
    "def fasterrcnn_reshape_transform(x):\n",
    "    \"\"\"\n",
    "    Reshape the output (activations) of a Faster R-CNN model to the shape of the input image.\n",
    "\n",
    "    Args:\n",
    "        x (dict): The output (activations) of a Faster R-CNN model.\n",
    "        \n",
    "    Returns:\n",
    "        dict: The reshaped output (activations) of a Faster R-CNN model.\n",
    "    \"\"\"\n",
    "    # Specify the target size (last two dimensions) of the reshaped output (height, width)\n",
    "    target_size = x['pool'].size()[-2 : ]\n",
    "    # Initialize an empty list for storing the activations\n",
    "    activations = []\n",
    "    # Iterate over the activations\n",
    "    for key, value in x.items():\n",
    "        # Resize the activation to the target size using bilinear interpolation\n",
    "        activations.append(torch.nn.functional.interpolate(torch.abs(value), target_size, mode='bilinear'))\n",
    "    # Concatenate the activations along the channel dimension\n",
    "    activations = torch.cat(activations, axis=1)\n",
    "    return activations\n",
    "\n",
    "class FasterRCNNBoxScoreTarget:\n",
    "    \"\"\" For every original detected bounding box specified in \"bounding boxes\",\n",
    "        assign a score on how the current bounding boxes match it,\n",
    "            1. In IOU\n",
    "            2. In the classification score.\n",
    "        If there is not a large enough overlap, or the category changed,\n",
    "        assign a score of 0.\n",
    "\n",
    "        The total score is the sum of all the box scores.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, labels, bounding_boxes, iou_threshold=0.5):\n",
    "        self.labels = labels\n",
    "        self.bounding_boxes = bounding_boxes\n",
    "        self.iou_threshold = iou_threshold\n",
    "\n",
    "    def __call__(self, model_outputs):\n",
    "        output = torch.Tensor([0])\n",
    "\n",
    "        if len(model_outputs[\"boxes\"]) == 0:\n",
    "            return output\n",
    "\n",
    "        for box, label in zip(self.bounding_boxes, self.labels):\n",
    "            box = torch.Tensor(box[None, :])\n",
    "\n",
    "            ious = torchvision.ops.box_iou(box, model_outputs[\"boxes\"])\n",
    "            index = ious.argmax()\n",
    "            if ious[0, index] > self.iou_threshold and model_outputs[\"labels\"][index] == label:\n",
    "                score = ious[0, index] + model_outputs[\"scores\"][index]\n",
    "                output = output + score\n",
    "        return output\n",
    "    \n",
    "def renormalize_cam_in_bounding_boxes(boxes, image_float_np, grayscale_cam):\n",
    "    \"\"\"Normalize the CAM to be in the range [0, 1] \n",
    "    inside every bounding boxes, and zero outside of the bounding boxes. \"\"\"\n",
    "    renormalized_cam = np.zeros(grayscale_cam.shape, dtype=np.float32)\n",
    "    images = []\n",
    "    for x1, y1, x2, y2 in boxes:\n",
    "        img = renormalized_cam * 0\n",
    "        img[y1:y2, x1:x2] = scale_cam_image(grayscale_cam[y1:y2, x1:x2].copy())    \n",
    "        images.append(img)\n",
    "    \n",
    "    renormalized_cam = np.max(np.float32(images), axis = 0)\n",
    "    renormalized_cam = scale_cam_image(renormalized_cam)\n",
    "    eigencam_image_renormalized = show_cam_on_image(image_float_np, renormalized_cam, use_rgb=True)\n",
    "    image_with_bounding_boxes = draw_boxes(boxes, labels, classes, eigencam_image_renormalized)\n",
    "    return image_with_bounding_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Inference & Evaluation (IoU & mAP)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `torchvision.models.detection` module provides several pre-trained models for object detection, segmentation, and person keypoint detection, as well as some training utilities. For Faster R-CNN, the following models are available:\n",
    "\n",
    "1. `fasterrcnn_resnet50_fpn`: This is a Faster R-CNN model with a ResNet-50 backbone and Feature Pyramid Network (FPN). It's pre-trained on the COCO train2017 dataset.\n",
    "\n",
    "2. `fasterrcnn_mobilenet_v3_large_fpn`: This is a Faster R-CNN model with a MobileNetV3-Large backbone and FPN. It's also pre-trained on the COCO train2017 dataset.\n",
    "\n",
    "3. `fasterrcnn_mobilenet_v3_large_320_fpn`: This is another Faster R-CNN model with a MobileNetV3-Large backbone and FPN, but designed for 320x320 input images. It's pre-trained on the COCO train2017 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_1_'></a>[Inference](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining constants\n",
    "DATASET_DIR = \"dataset/val2017/\"\n",
    "CONFIDENCE_THRESHOLD = 0.9\n",
    "SAVE = False\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Loading the COCO dataset in \"dataset/test2017\" \n",
    "test_images = os.listdir(DATASET_DIR)\n",
    "\n",
    "# Loading annotations\n",
    "coco_gt = COCO('dataset/instances_val2017.json')\n",
    "\n",
    "# Preparing the image info dictionary & filename to id dictionary\n",
    "image_ids = coco_gt.getImgIds()\n",
    "image_info = coco_gt.loadImgs(image_ids)\n",
    "filename_to_id = {img['file_name']: img['id'] for img in image_info}\n",
    "\n",
    "# Loading the pre-trained Faster R-CNN model trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Defining the transformation to be applied to images\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "split = \"val\"\n",
    "\n",
    "# Creating the output directory\n",
    "output_dir = \"output (Faster_RCNN)/\" + split + \"/\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Initializing empty lists for storing the predictions\n",
    "predictions = []\n",
    "\n",
    "# Iterating over the images\n",
    "for sample in test_images:\n",
    "\n",
    "    # Reading the image\n",
    "    img_id = filename_to_id[sample]\n",
    "    image = np.array(Image.open(DATASET_DIR + sample).convert('RGB'))\n",
    "    input_tensor = transform(image).unsqueeze(0)\n",
    "    input_tensor = input_tensor.to(DEVICE)\n",
    "\n",
    "    # Forward pass through the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)  \n",
    "\n",
    "    # Parse outputs\n",
    "    pred_boxes = outputs[0]['boxes'].data.cpu().numpy()\n",
    "    pred_scores = outputs[0]['scores'].data.cpu().numpy()\n",
    "    pred_labels = outputs[0]['labels'].data.cpu().numpy()\n",
    "\n",
    "    # Adding predictions to the list\n",
    "    for i, (box, score, label) in enumerate(zip(pred_boxes, pred_scores, pred_labels)):\n",
    "        box_height = box[3] - box[1]\n",
    "        box_width = box[2] - box[0]\n",
    "        new_box = np.array([box[0], box[1], box_width, box_height])\n",
    "        prediction = {\n",
    "            'image_id': img_id,\n",
    "            'category_id': int(label),\n",
    "            'bbox': new_box.tolist(),\n",
    "            'score': float(score)\n",
    "        }\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    # Drawing bounding boxes and class labels on the image and saving it iff SAVE is True\n",
    "    if SAVE:\n",
    "        boxes, classes, labels, indices = predict(input_tensor, model, CONFIDENCE_THRESHOLD)\n",
    "        image = draw_boxes(boxes, labels, classes, image)\n",
    "        Image.fromarray(image).save(output_dir + sample)\n",
    "\n",
    "# Saving the predictions in a JSON file\n",
    "with open(output_dir + 'predictions.json', 'w') as f:\n",
    "    json.dump(predictions, f)\n",
    "\n",
    "# Computing mAP\n",
    "coco_dt = coco_gt.loadRes(output_dir + 'predictions.json')\n",
    "coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
    "coco_eval.evaluate()\n",
    "coco_eval.accumulate()\n",
    "coco_eval.summarize()\n",
    "\n",
    "mAP = coco_eval.stats[0]\n",
    "print(\"mAP: \", mAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_2_'></a>[Computing IoU](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load COCO annotations for the entire dataset\n",
    "DATASET_DIR = \"/kaggle/input/coco-2017-dataset/coco2017/val2017/\"\n",
    "coco = COCO('/kaggle/input/coco-2017-dataset/coco2017/annotations/instances_val2017.json')\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "CONFIDENCE_THRESHOLD = 0.9\n",
    "\n",
    "# Load the pre-trained Faster R-CNN model trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Initializing variables\n",
    "total_iou = 0.0\n",
    "total_boxes = 0\n",
    "counter = 0\n",
    "\n",
    "# Iterating over all images in the dataset\n",
    "for img_id in coco.getImgIds():\n",
    "    image = read_image(DATASET_DIR, coco.loadImgs(img_id)[0]['file_name'])\n",
    "    input_tensor = transform(image).unsqueeze(0)\n",
    "    input_tensor = input_tensor.to(DEVICE)\n",
    "\n",
    "    # Getting ground truth boxes corresponding to the image id\n",
    "    ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "    anns = coco.loadAnns(ann_ids)\n",
    "    gt_boxes = []\n",
    "    gt_labels = []\n",
    "    for ann in anns:\n",
    "        gt_boxes.append(ann['bbox'])\n",
    "        gt_labels.append(ann['category_id'])\n",
    "    gt_boxes = np.array(gt_boxes)\n",
    "    gt_labels = np.array(gt_labels)\n",
    "\n",
    "    # Getting predicted boxes\n",
    "    boxes, classes, labels, indices, scores = predict_with_scores(input_tensor, model, CONFIDENCE_THRESHOLD)\n",
    "    keep_indices = [i for i, score in enumerate(scores) if score >= CONFIDENCE_THRESHOLD]\n",
    "\n",
    "    # only keep boxes with score >= CONFIDENCE_THRESHOLD\n",
    "    boxes = [boxes[i] for i in keep_indices]\n",
    "    labels = [labels[i] for i in keep_indices]\n",
    "\n",
    "    # Computing IoU\n",
    "    iou = 0.0\n",
    "    for index,gt_box in enumerate(gt_boxes):\n",
    "\n",
    "        # Get x2, y2 coordinates of the ground truth box\n",
    "        gt_x2 = gt_box[0] + gt_box[2]\n",
    "        gt_y2 = gt_box[1] + gt_box[3]\n",
    "        gt_box = [gt_box[0], gt_box[1], gt_x2, gt_y2]\n",
    "\n",
    "        max_iou = 0.0\n",
    "        for index_2, box in enumerate(boxes):\n",
    "            # computing ioU\n",
    "            x1 = max(gt_box[0], box[0])\n",
    "            y1 = max(gt_box[1], box[1])\n",
    "            x2 = min(gt_box[2], box[2])\n",
    "            y2 = min(gt_box[3], box[3])\n",
    "            intersection = max(x2 - x1, 0) * max(y2 - y1, 0)\n",
    "            union = (gt_box[2] - gt_box[0]) * (gt_box[3] - gt_box[1]) + (box[2] - box[0]) * (box[3] - box[1]) - intersection\n",
    "            iou = intersection / union\n",
    "            \n",
    "            if iou > max_iou and labels[index_2] == gt_labels[index]:\n",
    "                max_iou = iou\n",
    "        total_iou += max_iou\n",
    "        total_boxes += 1\n",
    "    if counter %100 ==0:\n",
    "        print(counter)\n",
    "    counter += 1\n",
    "\n",
    "print(\"Average IoU: \", total_iou.item() / total_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[Activation Visualization](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation visualization is a technique used in deep learning to understand and analyze the behavior of neural networks. It involves visualizing the activations or outputs of individual neurons or layers within a neural network when given a particular input.\n",
    "\n",
    "The main purpose of activation visualization is to gain insights into how the network is processing and representing the input data. By visualizing the activations, we can identify which parts of the input are being emphasized or ignored by the network. This can help us understand the network's decision-making process and potentially identify any issues or biases in the model. However, due to many number of filters and neurons in a neural network, it is not possible to visualize all the activations at once. Therefore, we typically visualize the activations of a single neuron or a small group of neurons at a time.\n",
    "\n",
    "In general, the process involves the following steps:\n",
    "\n",
    "1. Load the pre-trained model: Activation visualization is typically performed on pre-trained models. The first step is to load the model architecture and weights.\n",
    "\n",
    "2. Prepare the input data: Depending on the task, you need to prepare the input data that the model expects. This could involve preprocessing, resizing, or normalizing the input images.\n",
    "\n",
    "3. Forward pass: Pass the input data through the model to obtain the activations. This involves feeding the input data through the layers of the model and collecting the activations at the desired layer(s).\n",
    "\n",
    "4. Visualize the activations: Once you have obtained the activations, you can visualize them using various techniques such as heatmaps, feature maps, or activation histograms. These visualizations can provide insights into the learned representations and patterns within the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_visualization(images, model, transform, show=False, save=True, split='val'):\n",
    "    output_dir = \"output/activation_visualization/\" + split + \"/\"\n",
    "    global first_layer_activations\n",
    "    global last_layer_activations\n",
    "    \n",
    "    for sample in images:\n",
    "\n",
    "        # Read the image from disk using the image_name\n",
    "        image = read_image(DATASET_DIR, sample)\n",
    "        image_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        output = model(image_tensor)\n",
    "        \n",
    "        first_layer_activations = first_layer_activations.squeeze(0).detach().cpu().numpy()\n",
    "        last_layer_activations = last_layer_activations.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "        # Nomralize each of the activations\n",
    "        for i in range(first_layer_activations.shape[0]):\n",
    "            first_layer_activations[i] = (first_layer_activations[i] - first_layer_activations[i].min()) / (first_layer_activations[i].max() - first_layer_activations[i].min()) * 255\n",
    "\n",
    "        for i in range(last_layer_activations.shape[0]):\n",
    "            last_layer_activations[i] = (last_layer_activations[i] - last_layer_activations[i].min()) / (last_layer_activations[i].max() - last_layer_activations[i].min()) * 255\n",
    "           \n",
    "        # Visualize the activations\n",
    "        fig, axes = plt.subplots(3, 3, figsize=(10, 5))\n",
    "\n",
    "        # Original image\n",
    "        for i in range(0, 3):\n",
    "            axes[0, i].axis('off')\n",
    "        axes[0, 1].imshow(image)\n",
    "        axes[0, 1].set_title(\"Original Image\")\n",
    "\n",
    "        # First layer activations for only the first 3 filters\n",
    "        axes[1, 0].imshow(torchvision.transforms.ToPILImage()(first_layer_activations[0]), cmap=\"gray\")\n",
    "        axes[1, 0].set_title(\"First Layer Activations\\nFilter 1\")\n",
    "        axes[1, 0].axis(\"off\")\n",
    "\n",
    "        axes[1, 1].imshow(torchvision.transforms.ToPILImage()(first_layer_activations[1]), cmap=\"gray\")\n",
    "        axes[1, 1].set_title(\"First Layer Activations\\nFilter 2\")\n",
    "        axes[1, 1].axis(\"off\")\n",
    "\n",
    "        axes[1, 2].imshow(torchvision.transforms.ToPILImage()(first_layer_activations[2]), cmap=\"gray\")\n",
    "        axes[1, 2].set_title(\"First Layer Activations\\nFilter 3\")\n",
    "        axes[1, 2].axis(\"off\")\n",
    "\n",
    "        # Last layer activations for only the first 3 filters\n",
    "        axes[2, 0].imshow(torchvision.transforms.ToPILImage()(last_layer_activations[0]), cmap=\"gray\")\n",
    "        axes[2, 0].set_title(\"Last Layer Activations\\nFilter 1\")\n",
    "        axes[2, 0].axis(\"off\")\n",
    "\n",
    "        axes[2, 1].imshow(torchvision.transforms.ToPILImage()(last_layer_activations[1]), cmap=\"gray\")\n",
    "        axes[2, 1].set_title(\"Last Layer Activations\\nFilter 2\")\n",
    "        axes[2, 1].axis(\"off\")\n",
    "\n",
    "        axes[2, 2].imshow(torchvision.transforms.ToPILImage()(last_layer_activations[2]), cmap=\"gray\")\n",
    "        axes[2, 2].set_title(\"Last Layer Activations\\nFilter 3\")\n",
    "        axes[2, 2].axis(\"off\")\n",
    "        plt.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.05, hspace=0.2, wspace=0.2)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the figure iff needed\n",
    "        if save:\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            plt.savefig(output_dir + sample)\n",
    "\n",
    "        # Show the figure iff needed\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "        # Reset activations\n",
    "        first_layer_activations = None \n",
    "        last_layer_activations = None\n",
    "\n",
    "# Constants\n",
    "DATASET_DIR = \"dataset/val2017/\"\n",
    "\n",
    "# Load the pre-trained Faster R-CNN model trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Define the transformation to be applied to images\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "# Load the COCO dataset in \"dataset/test2017\"\n",
    "test_images = os.listdir(DATASET_DIR)\n",
    "\n",
    "# Specify the first and last layer of the model which will be used for visualization (hooked)\n",
    "first_layer = model.backbone.body.relu\n",
    "last_layer = model.backbone.body.layer4[2].conv3\n",
    "\n",
    "# Initializing activations which will be used in the hook functions\n",
    "first_layer_activations = None\n",
    "last_layer_activations = None\n",
    "\n",
    "# Defining hook for the first layer\n",
    "def first_layer_hook(module, input, output):\n",
    "    global first_layer_activations\n",
    "    first_layer_activations = output\n",
    "\n",
    "# Defining hook for the last layer\n",
    "def last_layer_hook(module, input, output):\n",
    "    global last_layer_activations\n",
    "    last_layer_activations = output\n",
    "\n",
    "# Registering the hooks\n",
    "first_layer.register_forward_hook(first_layer_hook)\n",
    "last_layer.register_forward_hook(last_layer_hook)\n",
    "\n",
    "# Apply activation visualization\n",
    "activation_visualization(test_images, model, transform, show=True, save=True, split='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_6_'></a>[GradCAM](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GradCAM (Gradient-Weighted Class Activaiton Mapping) introduced by the paper [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/abs/1610.02391) in 2017 is part of explainable AI for computer vision. It is a technique for visualizing and interpreting the predictions of Convolutional Neural Networks (CNNs). It uses the gradients of any target `concept` (say logits for 'dog' or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept.\n",
    "The GradCAM technique can be summarized in the following steps:\n",
    "\n",
    "1. Let $y^c$ be the score for class $c$ before the softmax, and $A^k$ be the activation map of the last convolutional layer. The gradient of $y^c$ w.r.t. $A^k$ is computed as:\n",
    "\n",
    "$$\\alpha^c_k = \\frac{1}{Z}\\sum_i\\sum_j\\frac{\\partial y^c}{\\partial A^k_{ij}}$$\n",
    "\n",
    "where $Z$ is the number of elements in $A^k$, and $A^k_{ij}$ is the activation at the $i$-th row and $j$-th column of $A^k$.\n",
    "\n",
    "2. The activation map $L^c_{GradCAM}$ is computed by:\n",
    "\n",
    "$$L^c_{GradCAM} = ReLU(\\sum_k\\alpha^c_kA^k)$$\n",
    "\n",
    "3. Heat map can then be computed by normalizing the activation map $L^c_{GradCAM}$.\n",
    "\n",
    "4. The heat map is then upsampled to the size of the input image.\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "- According to the authors, We find that Grad-CAM maps become progressively worse as we move to earlier convolutional layers as they have smaller receptive fields and only focus on less semantic local features. That is why most people tend to use the last convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"dataset/val2017/\"\n",
    "test_images = os.listdir(DATASET_DIR)\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "Gradcam = GradCAM(model,\n",
    "                    [model.backbone.body.layer4[2].conv3])\n",
    "\n",
    "for image in test_images:\n",
    "    image_name = image\n",
    "    image = np.array(Image.open(DATASET_DIR + image))\n",
    "    original_image = image.copy()\n",
    "    image_float_np = np.float32(image) / 255\n",
    "    \n",
    "    input_tensor = transform(image_float_np)\n",
    "    input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "    # Run the model and display the detections\n",
    "    boxes, classes, labels, indices = predict(input_tensor, model, 0.9)\n",
    "    targets = [FasterRCNNBoxScoreTarget(labels=labels, bounding_boxes=boxes)]\n",
    "\n",
    "    image_with_predictions = draw_boxes(boxes, labels, classes, image)\n",
    "\n",
    "    # Computing GradCAM\n",
    "    grayscale_gradcam = Gradcam(input_tensor=input_tensor, targets=targets)[0, :]\n",
    "    gradcam = show_cam_on_image(image_float_np, grayscale_gradcam, use_rgb=True)\n",
    "    if len(boxes) == 0:\n",
    "        continue\n",
    "    renormalized_gradcam = renormalize_cam_in_bounding_boxes(boxes, image_float_np, grayscale_gradcam)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(10, 5))\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title(\"Input Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    # Image with predicted bounding boxes\n",
    "    axes[1].imshow(image_with_predictions)\n",
    "    axes[1].set_title(\"Predicted Boxes\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    # GradCAM heatmap\n",
    "    axes[2].imshow(gradcam)\n",
    "    axes[2].set_title(\"GradCAM Heatmap\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    # GradCAM heatmap renormalized in bounding boxes\n",
    "    axes[3].imshow(renormalized_gradcam)\n",
    "    axes[3].set_title(\"GradCAM Heatmap\\nRenormalized in Bounding Boxes\")\n",
    "    axes[3].axis(\"off\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # Saving the images\n",
    "    output_dir = \"output/gradcam/val/\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    plt.savefig(output_dir + image_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_7_'></a>[GradCAM++](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grad-CAM++ introduced by Chattopadhyay et al in 2018 in their paper [Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks](https://arxiv.org/abs/1710.11063) is an extension of Grad-CAM. It refines the localization of important regions by considering higher-order derivatives beyond the first-order gradients. It combines the first and second-order gradients of the target concept to obtain the weights (specifically, the Hessian matrix) for the activation maps. This refinement aims to enhance the localization accuracy of the highlighted regions, providing a more precise understanding of where the model is focusing its attention to make predictions. It is worth noting that the paper offers a concrete mathematical derivation of the Grad-CAM++ which can't be covered here.\n",
    "\n",
    "![Image](assets/gradcam%20vs%20gradcampp.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"dataset/val2017/\"\n",
    "test_images = os.listdir(DATASET_DIR)\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "Gradcampp = GradCAMPlusPlus(model,\n",
    "                    [model.backbone.body.layer4[2].conv3])\n",
    "\n",
    "for image in test_images:\n",
    "    image_name = image\n",
    "    image = np.array(Image.open(DATASET_DIR + image))\n",
    "    original_image = image.copy()\n",
    "    image_float_np = np.float32(image) / 255\n",
    "    \n",
    "    input_tensor = transform(image_float_np)\n",
    "    input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "    # Run the model and display the detections\n",
    "    boxes, classes, labels, indices = predict(input_tensor, model, 0.9)\n",
    "    if len(boxes) == 0:\n",
    "        continue\n",
    "    targets = [FasterRCNNBoxScoreTarget(labels=labels, bounding_boxes=boxes)]\n",
    "\n",
    "    image_with_predictions = draw_boxes(boxes, labels, classes, image)\n",
    "\n",
    "    # Computing GradCAM\n",
    "    grayscale_gradcam = Gradcampp(input_tensor=input_tensor, targets=targets)[0, :]\n",
    "    gradcam = show_cam_on_image(image_float_np, grayscale_gradcam, use_rgb=True)\n",
    "    if len(boxes) == 0:\n",
    "        continue\n",
    "    renormalized_gradcam = renormalize_cam_in_bounding_boxes(boxes, image_float_np, grayscale_gradcam)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(10, 5))\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title(\"Input Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    # Image with predicted bounding boxes\n",
    "    axes[1].imshow(image_with_predictions)\n",
    "    axes[1].set_title(\"Predicted Boxes\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    # GradCAM heatmap\n",
    "    axes[2].imshow(gradcam)\n",
    "    axes[2].set_title(\"GradCAM++ Heatmap\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    # GradCAM heatmap renormalized in bounding boxes\n",
    "    axes[3].imshow(renormalized_gradcam)\n",
    "    axes[3].set_title(\"GradCAM++ Heatmap\\nRenormalized in Bounding Boxes\")\n",
    "    axes[3].axis(\"off\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # Saving the images\n",
    "    output_dir = \"output/gradcampp/val/\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    plt.savefig(output_dir + image_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_8_'></a>[EigenCAM](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EigenCAM is a class activation mapping technique that provides visual explanations for convolutional neural network (CNN) models. It was introduced by Mohammed Bany Muhammad Mohammed Yeasin in their paper [Eigen-CAM: Class Activation Map using Principal Components](https://arxiv.org/abs/2008.00299). EigenCAM is a generalization of GradCAM and GradCAM++ that uses the principal components using Singular Value Decomposition of the activation maps to compute the weights for the activation maps. This allows EigenCAM to capture the most important features in the activation maps, providing a more accurate localization of the important regions in the input image.\n",
    "\n",
    "The main advantage is the ability to give robust explanations for the model's predictions even if it is misclassified. This is because EigenCAM uses the principal components of the activation maps, which are not affected by the model's predictions and doesn't require gradient information. This is in contrast to GradCAM and GradCAM++, which use the gradients of the model's predictions to compute the weights for the activation maps. This means that GradCAM and GradCAM++ can give misleading explanations if the model's predictions are incorrect.\n",
    "\n",
    "![Image](assets/eigenmap%20vs%20gradcam.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"dataset/val2017/\"\n",
    "test_images = os.listdir(DATASET_DIR)\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "target_layers = [model.backbone]\n",
    "\n",
    "Eigencam = EigenCAM(model,\n",
    "               target_layers, \n",
    "               reshape_transform=fasterrcnn_reshape_transform)\n",
    "\n",
    "for image in test_images:\n",
    "    image_name = image\n",
    "    image = np.array(Image.open(DATASET_DIR + image))\n",
    "    original_image = image.copy()\n",
    "    image_float_np = np.float32(image) / 255\n",
    "    \n",
    "    input_tensor = transform(image_float_np)\n",
    "    input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "    # Run the model and display the detections\n",
    "    boxes, classes, labels, indices = predict(input_tensor, model, 0.9)\n",
    "    targets = [FasterRCNNBoxScoreTarget(labels=labels, bounding_boxes=boxes)]\n",
    "\n",
    "    image_with_predictions = draw_boxes(boxes, labels, classes, image)\n",
    "\n",
    "    # Computing EigenCAM\n",
    "    grayscale_eigencam = Eigencam(input_tensor=input_tensor, targets=targets)[0, :]\n",
    "    eigencam_image = show_cam_on_image(image_float_np, grayscale_eigencam, use_rgb=True)\n",
    "    if len(boxes) == 0:\n",
    "        continue\n",
    "    renormalized_eigencam_image = renormalize_cam_in_bounding_boxes(boxes, image_float_np, grayscale_eigencam)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(10, 5))\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title(\"Input Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    # Image with predicted bounding boxes\n",
    "    axes[1].imshow(image_with_predictions)\n",
    "    axes[1].set_title(\"Predicted Boxes\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    # EigenCAM heatmap\n",
    "    axes[2].imshow(eigencam_image)\n",
    "    axes[2].set_title(\"EigenCAM Heatmap\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    # EigenCAM heatmap renormalized in bounding boxes\n",
    "    axes[3].imshow(renormalized_eigencam_image)\n",
    "    axes[3].set_title(\"EigenCAM Heatmap\\nRenormalized in Bounding Boxes\")\n",
    "    axes[3].axis(\"off\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Saving the figures\n",
    "    output_dir = \"output/eigencam/val/\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    plt.savefig(output_dir + image_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_9_'></a>[AblationCAM](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AblationCAM introduced by Saurabh Desai and Harish Ramaswamy in their paper [Ablation-CAM: Visual Explanations for Deep Convolutional Network via Gradient-free Localization](https://openaccess.thecvf.com/content_WACV_2020/papers/Desai_Ablation-CAM_Visual_Explanations_for_Deep_Convolutional_Network_via_Gradient-free_Localization_WACV_2020_paper.pdf) which was published in CVPR 2020 is a gradient-free class activation mapping technique that provides visual explanations for convolutional neural network (CNN) models. It is a gradient-free alternative to GradCAM and GradCAM++ that uses ablation to compute the weights for the activation maps. This allows AblationCAM to provide visual explanations for models that do not have a gradient defined for their output.\n",
    "\n",
    "The main idea is to use ablation (zero out) to compute the weights for the activation maps. This involves removing each activation map one at a time and computing the difference in the model's predictions. The weights are then computed by taking the difference between the model's predictions with and without the activation map. These weights are then used to compute the activation map, which is then upsampled to the size of the input image.\n",
    "\n",
    "The ablation impact directly measures the importance of a unit to the class score, rather than using gradients which are indirect and noisy. It is also insensitive to implementation details like model architecture. The ablation impact remains consistent across models. Ablation-CAM explanations do not change drastically for wrong predictions. Moreover, Gradient-based methods can highlight unrelated regions if the model is wrong.\n",
    "Computationally, not backpropagating gradients can be more efficient for generating explanations.\n",
    "\n",
    "![Image](assets/ablationcam%20vs%20gradcam.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"dataset/val2017/\"\n",
    "test_images = os.listdir(DATASET_DIR)\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "target_layers = [model.backbone]\n",
    "\n",
    "Ablationcam = AblationCAM(model,\n",
    "                  target_layers, \n",
    "                  reshape_transform=fasterrcnn_reshape_transform,\n",
    "                  ablation_layer=AblationLayerFasterRCNN(),\n",
    "                  ratio_channels_to_ablate=0.01)\n",
    "\n",
    "for image in test_images:\n",
    "    image_name = image\n",
    "    image = np.array(Image.open(DATASET_DIR + image))\n",
    "    original_image = image.copy()\n",
    "    image_float_np = np.float32(image) / 255\n",
    "    \n",
    "    input_tensor = transform(image_float_np)\n",
    "    input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "    # Run the model and display the detections\n",
    "    boxes, classes, labels, indices = predict(input_tensor, model, 0.9)\n",
    "    targets = [FasterRCNNBoxScoreTarget(labels=labels, bounding_boxes=boxes)]\n",
    "\n",
    "    image_with_predictions = draw_boxes(boxes, labels, classes, image)\n",
    "\n",
    "    # Computing AblationCAM\n",
    "    grayscale_ablationcam = Ablationcam(input_tensor=input_tensor, targets=targets)[0, :]\n",
    "    ablationcam = show_cam_on_image(image_float_np, grayscale_ablationcam, use_rgb=True)\n",
    "    if len(boxes) == 0:\n",
    "        continue\n",
    "    renormalized_ablationcam = renormalize_cam_in_bounding_boxes(boxes, image_float_np, grayscale_ablationcam)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(10, 5))\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title(\"Input Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    # Image with predicted bounding boxes\n",
    "    axes[1].imshow(image_with_predictions)\n",
    "    axes[1].set_title(\"Predicted Boxes\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    # AblationCAM heatmap\n",
    "    axes[2].imshow(ablationcam)\n",
    "    axes[2].set_title(\"AblationCAM Heatmap\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    # AblationCAM heatmap renormalized in bounding boxes\n",
    "    axes[3].imshow(renormalized_ablationcam)\n",
    "    axes[3].set_title(\"AblationCAM Heatmap\\nRenormalized in Bounding Boxes\")\n",
    "    axes[3].axis(\"off\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # Saving the images\n",
    "    output_dir = \"output/ablation/val/\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    plt.savefig(output_dir + image_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_10_'></a>[Deep Feature Factorizations.](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Feature Factorization -DFF for short-, a method capable of localizing similar semantic concepts within an image or a set of images. It was introduced by Collins el al in their paper [Deep Feature Factorization For Concept Discovery](https://arxiv.org/abs/1806.10206).\n",
    "\n",
    "Usually explainability methods answer questions like “Where does the model see a cat in the image?”. Here instead we will get a much more detailed glimpse into the model, and ask it: “Show me all the different concepts you found inside the image, and how are they classified”.\n",
    "\n",
    "\n",
    "The previous methods were not able to answer a lot of questions like:\n",
    "\n",
    "\n",
    "1. What are the internal concepts the model finds?\n",
    "\n",
    "Does the network just see the cat head and body together? Or maybe it detects them as different concepts ? We heard that neural networks are able to identify high level features like ears, eyes, faces and legs. But we’re never actually able to see this in the model explanations.\n",
    "\n",
    "2. Could it be that the body of the cat also pulls the output towards other categories as well?\n",
    "\n",
    "Just because it contributes to a higher output for one category, it doesn’t mean it doesn’t contribute to other categories as well. For example, there are many different types of cats. To take this into account when we’re interpreting the heatmaps, we would have to carefully look at all the heatmaps and keep track of them.\n",
    "\n",
    "3. How do we merge all the visualizations into a single image?\n",
    "\n",
    "In terms of the visualization itself, if we have 10 heatmaps for 10 categories, we would need to look at 10 different images. And some of the pixels could get high values in several heatmaps, for example different categories of cats. This is a lot of information to unpack and not very effecient.\n",
    "\n",
    "\n",
    "The idea of DFF is to factorize the activations from the model into different concepts using Non Negative Matrix Factorization (or from now on- NMF), and for every pixel compute how it corresponds with each of the concepts:\n",
    "\n",
    "- Turn the small 2D images in activations into 1D vectors, by reshaping the activations from a tensor of shape Batch, Channels, Height , Width, to a tensor with the shape Channels x (Batch x Height x Width) Reminder: the activations are typcically non-negative since they are often after a ReLU gate.\n",
    "\n",
    "- Compute the NMF of V, for some number of components N.\n",
    "\n",
    "This gives us V = WH.\n",
    "\n",
    "W is a matrix with the shape (channels x N).\n",
    "\n",
    "H is a matrix with the shape N x (Batch x Height x Width).\n",
    "\n",
    "- W can be thought of as the feature representations of the detected concepts.\n",
    "\n",
    "- H (after reshaping it back to 2D activations) contains how the pixels corresponds with the different concepts.\n",
    "\n",
    "If we input a batch of several images, concepts that repeat across the images will be computed. This gives us a way of automatically discovering concepts in a dataset, and performing tasks like co-localization, further detailed in the paper. However for our purposes now we will use a batch size of 1: we just want to detect the concepts detected in a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"dataset/val2017/\"\n",
    "test_images = os.listdir(DATASET_DIR)\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "dff = DeepFeatureFactorization(model=model, target_layer=model.backbone.body.layer3[5].conv3, computation_on_concepts=model.roi_heads.box_predictor.cls_score)\n",
    "\n",
    "\n",
    "for image in test_images:\n",
    "    image_name = image\n",
    "    image = np.array(Image.open(DATASET_DIR + image))\n",
    "    original_image = image.copy()\n",
    "    image_float_np = np.float32(image) / 255\n",
    "    \n",
    "    input_tensor = transform(image_float_np)\n",
    "    input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "    # Run the model and display the detections\n",
    "    boxes, classes, labels, indices = predict(input_tensor, model, 0.9)\n",
    "\n",
    "    image_with_predictions = draw_boxes(boxes, labels, classes, image)\n",
    "    \n",
    "    # Computing Deep Feature Factorization\n",
    "    concepts_2, batch_explanations_2, concept_scores_2 = dff(input_tensor, n_components = 2)\n",
    "    visualization_2 = show_factorization_on_image(image_float_np, \n",
    "                                                batch_explanations_2[0],\n",
    "                                                image_weight=0.3)\n",
    "    \n",
    "    # Computing again for n_components = 3\n",
    "    concepts_3, batch_explanations_3, concept_scores_3 = dff(input_tensor, n_components = 3)\n",
    "    visualization_3 = show_factorization_on_image(image_float_np, \n",
    "                                                batch_explanations_3[0],\n",
    "                                                image_weight=0.3)\n",
    "    \n",
    "    # Computing again for n_components = 5\n",
    "    concepts_5, batch_explanations_5, concept_scores_5 = dff(input_tensor, n_components = 5)\n",
    "    visualization_5 = show_factorization_on_image(image_float_np, \n",
    "                                                batch_explanations_5[0],\n",
    "                                                image_weight=0.3)\n",
    "        \n",
    "    if len(boxes) == 0:\n",
    "        continue\n",
    "\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(10, 5))\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title(\"Input Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "    \n",
    "    # Image with predicted bounding boxes\n",
    "    axes[1].imshow(image_with_predictions)\n",
    "    axes[1].set_title(\"Predicted Boxes\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    # Deep Feature Factorization heatmap\n",
    "    axes[2].imshow(visualization_2)\n",
    "    axes[2].set_title(\"DFF Heatmap (2)\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    # Deep Feature Factorization heatmap\n",
    "    axes[3].imshow(visualization_3)\n",
    "    axes[3].set_title(\"DFF Heatmap (3)\")\n",
    "    axes[3].axis(\"off\")\n",
    "\n",
    "    # Deep Feature Factorization heatmap\n",
    "    axes[4].imshow(visualization_5)\n",
    "    axes[4].set_title(\"DFF Heatmap (5)\")\n",
    "    axes[4].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # Saving the images\n",
    "    output_dir = \"output/dff/val/\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    plt.savefig(output_dir + image_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_11_'></a>[ScoreCAM](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So far it causes the computer to crash. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"dataset/val2017/\"\n",
    "test_images = os.listdir(DATASET_DIR)\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "target_layers = [model.backbone.body.layer4[2].conv3]\n",
    "\n",
    "Scorecam = ScoreCAM(model,\n",
    "                target_layers)\n",
    "\n",
    "for image in test_images:\n",
    "    image_name = image\n",
    "    image = np.array(Image.open(DATASET_DIR + image))\n",
    "    original_image = image.copy()\n",
    "    image_float_np = np.float32(image) / 255\n",
    "    \n",
    "    input_tensor = transform(image_float_np)\n",
    "    input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "    # Run the model and display the detections\n",
    "    boxes, classes, labels, indices = predict(input_tensor, model, 0.9)\n",
    "    targets = [FasterRCNNBoxScoreTarget(labels=labels, bounding_boxes=boxes)]\n",
    "\n",
    "    image_with_predictions = draw_boxes(boxes, labels, classes, image)\n",
    "\n",
    "    # Computing ScoreCAM\n",
    "    grayscale_scorecam = Scorecam(input_tensor=input_tensor, targets=targets)[0, :]\n",
    "    scorecam = show_cam_on_image(image_float_np, grayscale_scorecam, use_rgb=True)\n",
    "    if len(boxes) == 0:\n",
    "        continue\n",
    "    renormalized_scorecam = renormalize_cam_in_bounding_boxes(boxes, image_float_np, grayscale_scorecam)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(10, 5))\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title(\"Input Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    # Image with predicted bounding boxes\n",
    "    axes[1].imshow(image_with_predictions)\n",
    "    axes[1].set_title(\"Predicted Boxes\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    # ScoreCAM heatmap\n",
    "    axes[2].imshow(scorecam)\n",
    "    axes[2].set_title(\"ScoreCAM Heatmap\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    # ScoreCAM heatmap renormalized in bounding boxes\n",
    "    axes[3].imshow(renormalized_scorecam)\n",
    "    axes[3].set_title(\"ScoreCAM Heatmap\\nRenormalized in Bounding Boxes\")\n",
    "    axes[3].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # Saving the images\n",
    "    output_dir = \"output/scorecam/val/\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    plt.savefig(output_dir + image_name)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
