{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "from pytorch_grad_cam.ablation_layer import AblationLayerFasterRCNN\n",
    "from pytorch_grad_cam import GradCAM, AblationCAM, EigenCAM, ScoreCAM, GradCAMPlusPlus\n",
    "from pytorch_grad_cam.utils.model_targets import FasterRCNNBoxScoreTarget\n",
    "from pytorch_grad_cam.utils.reshape_transforms import fasterrcnn_reshape_transform\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, scale_cam_image\n",
    "\n",
    "coco_labels = ['__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', \\\n",
    "              'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', \n",
    "              'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', \n",
    "              'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella',\n",
    "              'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n",
    "              'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
    "              'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass', 'cup', 'fork',\n",
    "              'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n",
    "              'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "              'potted plant', 'bed', 'N/A', 'dining table', 'N/A', 'N/A', 'toilet',\n",
    "              'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n",
    "              'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book', 'clock', 'vase',\n",
    "              'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
    "coco_names = coco_labels.copy()\n",
    "              \n",
    "def read_image(dataset_dir, image_name):\n",
    "    image = cv2.imread(dataset_dir + image_name)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "def plot_boxes(image, boxes, labels, scores, coco_labels):\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        box = [int(coord) for coord in box]\n",
    "        score = float(score)\n",
    "        cv2.rectangle(image, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "        # Convert label ID to corresponding class name\n",
    "        class_name = coco_labels[label]\n",
    "        if class_name == \"N/A\":\n",
    "            continue\n",
    "        text = \"{}: {:.2f}\".format(class_name, score)\n",
    "        cv2.putText(image, text, (box[0], box[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    return image\n",
    "\n",
    "def predict_boxes(model, image_tensor, confidence_threshold, iou_threshold):\n",
    "    # Forward pass through the model\n",
    "    output = model(image_tensor)\n",
    "\n",
    "    # Visualize predictions on the image\n",
    "    boxes, labels, scores = output[0]['boxes'], output[0]['labels'], output[0]['scores']\n",
    "\n",
    "    # Apply confidence thresholding\n",
    "    keep = scores > confidence_threshold\n",
    "    boxes, labels, scores = boxes[keep], labels[keep], scores[keep]\n",
    "\n",
    "    # Perform non-maximum suppression using NMS\n",
    "    keep_idxs = torchvision.ops.nms(boxes, scores, iou_threshold=iou_threshold)\n",
    "\n",
    "    # Apply NMS to retain the most confident detections\n",
    "    boxes, labels, scores = boxes[keep_idxs], labels[keep_idxs], scores[keep_idxs]\n",
    "\n",
    "    return boxes, labels, scores\n",
    "\n",
    "def predict(input_tensor, model, detection_threshold):\n",
    "    outputs = model(input_tensor)\n",
    "    pred_classes = [coco_names[i] for i in outputs[0]['labels'].cpu().numpy()]\n",
    "    pred_labels = outputs[0]['labels'].cpu().numpy()\n",
    "    pred_scores = outputs[0]['scores'].detach().cpu().numpy()\n",
    "    pred_bboxes = outputs[0]['boxes'].detach().cpu().numpy()\n",
    "    \n",
    "    boxes, classes, labels, indices = [], [], [], []\n",
    "    for index in range(len(pred_scores)):\n",
    "        if pred_scores[index] >= detection_threshold:\n",
    "            boxes.append(pred_bboxes[index].astype(np.int32))\n",
    "            classes.append(pred_classes[index])\n",
    "            labels.append(pred_labels[index])\n",
    "            indices.append(index)\n",
    "    boxes = np.int32(boxes)\n",
    "    return boxes, classes, labels, indices\n",
    "\n",
    "def predict_with_scores(input_tensor, model, detection_threshold):\n",
    "    outputs = model(input_tensor)\n",
    "    pred_classes = [coco_names[i] for i in outputs[0]['labels'].cpu().numpy()]\n",
    "    pred_labels = outputs[0]['labels'].cpu().numpy()\n",
    "    pred_scores = outputs[0]['scores'].detach().cpu().numpy()\n",
    "    pred_bboxes = outputs[0]['boxes'].detach().cpu().numpy()\n",
    "    \n",
    "    boxes, classes, labels, indices = [], [], [], []\n",
    "    for index in range(len(pred_scores)):\n",
    "        if pred_scores[index] >= detection_threshold:\n",
    "            boxes.append(pred_bboxes[index].astype(np.int32))\n",
    "            classes.append(pred_classes[index])\n",
    "            labels.append(pred_labels[index])\n",
    "            indices.append(index)\n",
    "    boxes = np.int32(boxes)\n",
    "    return boxes, classes, labels, indices, outputs\n",
    "\n",
    "\n",
    "def draw_boxes(boxes, labels, classes, image):\n",
    "    for i, box in enumerate(boxes):\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (int(box[0]), int(box[1])),\n",
    "            (int(box[2]), int(box[3])),\n",
    "            (0, 255, 0), 2\n",
    "        )\n",
    "        cv2.putText(image, classes[i], (int(box[0]), int(box[1] - 5)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2,\n",
    "                    lineType=cv2.LINE_AA)\n",
    "    return image\n",
    "\n",
    "def fasterrcnn_reshape_transform(x):\n",
    "    target_size = x['pool'].size()[-2 : ]\n",
    "    activations = []\n",
    "    for key, value in x.items():\n",
    "        activations.append(torch.nn.functional.interpolate(torch.abs(value), target_size, mode='bilinear'))\n",
    "    activations = torch.cat(activations, axis=1)\n",
    "    return activations\n",
    "\n",
    "class FasterRCNNBoxScoreTarget:\n",
    "    \"\"\" For every original detected bounding box specified in \"bounding boxes\",\n",
    "        assign a score on how the current bounding boxes match it,\n",
    "            1. In IOU\n",
    "            2. In the classification score.\n",
    "        If there is not a large enough overlap, or the category changed,\n",
    "        assign a score of 0.\n",
    "\n",
    "        The total score is the sum of all the box scores.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, labels, bounding_boxes, iou_threshold=0.5):\n",
    "        self.labels = labels\n",
    "        self.bounding_boxes = bounding_boxes\n",
    "        self.iou_threshold = iou_threshold\n",
    "\n",
    "    def __call__(self, model_outputs):\n",
    "        output = torch.Tensor([0])\n",
    "\n",
    "        if len(model_outputs[\"boxes\"]) == 0:\n",
    "            return output\n",
    "\n",
    "        for box, label in zip(self.bounding_boxes, self.labels):\n",
    "            box = torch.Tensor(box[None, :])\n",
    "\n",
    "            ious = torchvision.ops.box_iou(box, model_outputs[\"boxes\"])\n",
    "            index = ious.argmax()\n",
    "            if ious[0, index] > self.iou_threshold and model_outputs[\"labels\"][index] == label:\n",
    "                score = ious[0, index] + model_outputs[\"scores\"][index]\n",
    "                output = output + score\n",
    "        return output\n",
    "    \n",
    "def renormalize_cam_in_bounding_boxes(boxes, image_float_np, grayscale_cam):\n",
    "    \"\"\"Normalize the CAM to be in the range [0, 1] \n",
    "    inside every bounding boxes, and zero outside of the bounding boxes. \"\"\"\n",
    "    renormalized_cam = np.zeros(grayscale_cam.shape, dtype=np.float32)\n",
    "    images = []\n",
    "    for x1, y1, x2, y2 in boxes:\n",
    "        img = renormalized_cam * 0\n",
    "        img[y1:y2, x1:x2] = scale_cam_image(grayscale_cam[y1:y2, x1:x2].copy())    \n",
    "        images.append(img)\n",
    "    \n",
    "    renormalized_cam = np.max(np.float32(images), axis = 0)\n",
    "    renormalized_cam = scale_cam_image(renormalized_cam)\n",
    "    eigencam_image_renormalized = show_cam_on_image(image_float_np, renormalized_cam, use_rgb=True)\n",
    "    image_with_bounding_boxes = draw_boxes(boxes, labels, classes, eigencam_image_renormalized)\n",
    "    return image_with_bounding_boxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference & Evaluation (IoU & mAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `torchvision.models.detection` module provides several pre-trained models for object detection, segmentation, and person keypoint detection, as well as some training utilities. For Faster R-CNN, the following models are available:\n",
    "\n",
    "1. `fasterrcnn_resnet50_fpn`: This is a Faster R-CNN model with a ResNet-50 backbone and Feature Pyramid Network (FPN). It's pre-trained on the COCO train2017 dataset.\n",
    "\n",
    "2. `fasterrcnn_mobilenet_v3_large_fpn`: This is a Faster R-CNN model with a MobileNetV3-Large backbone and FPN. It's also pre-trained on the COCO train2017 dataset.\n",
    "\n",
    "3. `fasterrcnn_mobilenet_v3_large_320_fpn`: This is another Faster R-CNN model with a MobileNetV3-Large backbone and FPN, but designed for 320x320 input images. It's pre-trained on the COCO train2017 dataset.\n",
    "\n",
    "You can use these models like this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(images, model, transform, confidence_threshold, coco_labels, show=False, save=True, split='val'):\n",
    "\n",
    "    output_dir = \"output/\" + split + \"/confidence_threshold_\" + str(confidence_threshold) + \"/\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for sample in images:\n",
    "        image = np.array(Image.open(DATASET_DIR + sample))\n",
    "        image_float_np = np.float32(image) / 255\n",
    "        \n",
    "        input_tensor = transform(image_float_np)\n",
    "        input_tensor = input_tensor.unsqueeze(0)\n",
    "        \n",
    "        boxes, classes, labels, _ = predict(input_tensor, model, confidence_threshold)\n",
    "        image = draw_boxes(boxes, labels, classes, image)\n",
    "        \n",
    "        if show:\n",
    "            cv2.imshow(\"Image\", image)\n",
    "            cv2.waitKey(0)\n",
    "\n",
    "        if save:\n",
    "            cv2.imwrite(output_dir + sample, image)\n",
    "\n",
    "\n",
    "# Constants\n",
    "DATASET_DIR = \"dataset/val2017/\"\n",
    "CONFIDENCE_THRESHOLD = 0.9\n",
    "IOU_THRESHOLD = 0.5\n",
    "\n",
    "# Load the COCO dataset in \"dataset/test2017\" \n",
    "test_images = os.listdir(DATASET_DIR)\n",
    "\n",
    "# Load the pre-trained Faster R-CNN model trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "model.eval()\n",
    "\n",
    "# Define the transformation to be applied to images\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "# Applying inference on the images\n",
    "inference(test_images, model, transform, CONFIDENCE_THRESHOLD, coco_labels, show=False, save=True, split='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation visualization is a technique used in deep learning to understand and analyze the behavior of neural networks. It involves visualizing the activations or outputs of individual neurons or layers within a neural network when given a particular input.\n",
    "\n",
    "The main purpose of activation visualization is to gain insights into how the network is processing and representing the input data. By visualizing the activations, we can identify which parts of the input are being emphasized or ignored by the network. This can help us understand the network's decision-making process and potentially identify any issues or biases in the model.\n",
    "\n",
    "In general, the process involves the following steps:\n",
    "\n",
    "1. Load the pre-trained model: Activation visualization is typically performed on pre-trained models. The first step is to load the model architecture and weights.\n",
    "\n",
    "2. Prepare the input data: Depending on the task, you need to prepare the input data that the model expects. This could involve preprocessing, resizing, or normalizing the input images.\n",
    "\n",
    "3. Forward pass: Pass the input data through the model to obtain the activations. This involves feeding the input data through the layers of the model and collecting the activations at the desired layer(s).\n",
    "\n",
    "4. Visualize the activations: Once you have obtained the activations, you can visualize them using various techniques such as heatmaps, feature maps, or activation histograms. These visualizations can provide insights into the learned representations and patterns within the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_visualization(images, model, transform, show=False, save=True, split='val'):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    output_dir = \"output/activation_visualization/\" + split + \"/\"\n",
    "    global first_layer_activations\n",
    "    global last_layer_activations\n",
    "    \n",
    "    for sample in images:\n",
    "\n",
    "        # Read the image from disk using the image_name\n",
    "        image_name = sample\n",
    "        image = read_image(DATASET_DIR, image_name)\n",
    "        image_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        output = model(image_tensor)\n",
    "        \n",
    "        first_layer_activations = first_layer_activations.squeeze(0).detach().cpu().numpy()\n",
    "        last_layer_activations = last_layer_activations.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "        # Nomralize each of the activations\n",
    "        for i in range(first_layer_activations.shape[0]):\n",
    "            first_layer_activations[i] = (first_layer_activations[i] - first_layer_activations[i].min()) / (first_layer_activations[i].max() - first_layer_activations[i].min()) * 255\n",
    "\n",
    "        for i in range(last_layer_activations.shape[0]):\n",
    "            last_layer_activations[i] = (last_layer_activations[i] - last_layer_activations[i].min()) / (last_layer_activations[i].max() - last_layer_activations[i].min()) * 255\n",
    "           \n",
    "        # Visualize the activations\n",
    "        # create a figure with three rows and three columns, first row is the original image (only one image)\n",
    "        fig, axes = plt.subplots(3, 3, figsize=(10, 5))\n",
    "\n",
    "        # Original image\n",
    "        for i in range(0, 3):\n",
    "            axes[0, i].axis('off')\n",
    "        axes[0, 1].imshow(image)\n",
    "        axes[0, 1].set_title(\"Original Image\")\n",
    "\n",
    "        # First layer activations for only the first 3 filters\n",
    "        axes[1, 0].imshow(torchvision.transforms.ToPILImage()(first_layer_activations[0]), cmap=\"gray\")\n",
    "        axes[1, 0].set_title(\"First Layer Activations\\nFilter 1\")\n",
    "        axes[1, 0].axis(\"off\")\n",
    "\n",
    "        axes[1, 1].imshow(torchvision.transforms.ToPILImage()(first_layer_activations[1]), cmap=\"gray\")\n",
    "        axes[1, 1].set_title(\"First Layer Activations\\nFilter 2\")\n",
    "        axes[1, 1].axis(\"off\")\n",
    "\n",
    "        axes[1, 2].imshow(torchvision.transforms.ToPILImage()(first_layer_activations[2]), cmap=\"gray\")\n",
    "        axes[1, 2].set_title(\"First Layer Activations\\nFilter 3\")\n",
    "        axes[1, 2].axis(\"off\")\n",
    "\n",
    "        # Last layer activations for only the first 3 filters\n",
    "        axes[2, 0].imshow(torchvision.transforms.ToPILImage()(last_layer_activations[0]), cmap=\"gray\")\n",
    "        axes[2, 0].set_title(\"Last Layer Activations\\nFilter 1\")\n",
    "        axes[2, 0].axis(\"off\")\n",
    "\n",
    "        axes[2, 1].imshow(torchvision.transforms.ToPILImage()(last_layer_activations[1]), cmap=\"gray\")\n",
    "        axes[2, 1].set_title(\"Last Layer Activations\\nFilter 2\")\n",
    "        axes[2, 1].axis(\"off\")\n",
    "\n",
    "        axes[2, 2].imshow(torchvision.transforms.ToPILImage()(last_layer_activations[2]), cmap=\"gray\")\n",
    "        axes[2, 2].set_title(\"Last Layer Activations\\nFilter 3\")\n",
    "        axes[2, 2].axis(\"off\")\n",
    "        plt.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.05, hspace=0.2, wspace=0.2)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            plt.savefig(output_dir + image_name)\n",
    "\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "        first_layer_activations = None \n",
    "        last_layer_activations = None\n",
    "\n",
    "# Constants\n",
    "DATASET_DIR = \"dataset/val2017/\"\n",
    "print(\"Loaded {} labels from COCO dataset\".format(len(coco_labels)))\n",
    "\n",
    "# Load the pre-trained Faster R-CNN model trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Define the transformation to be applied to images\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "# Load the COCO dataset in \"dataset/test2017\"\n",
    "test_images = os.listdir(DATASET_DIR)\n",
    "\n",
    "first_layer = model.backbone.body.relu\n",
    "last_layer = model.backbone.body.layer4[2].conv3\n",
    "\n",
    "first_layer_activations = None\n",
    "last_layer_activations = None\n",
    "\n",
    "# Defining hook for the first layer\n",
    "def first_layer_hook(module, input, output):\n",
    "    global first_layer_activations\n",
    "    first_layer_activations = output\n",
    "\n",
    "# Defining hook for the last layer\n",
    "def last_layer_hook(module, input, output):\n",
    "    global last_layer_activations\n",
    "    last_layer_activations = output\n",
    "\n",
    "# Registering the hooks\n",
    "first_layer.register_forward_hook(first_layer_hook)\n",
    "last_layer.register_forward_hook(last_layer_hook)\n",
    "\n",
    "# Apply activation visualization\n",
    "activation_visualization(test_images, model, transform, show=True, save=True, split='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "DATASET_DIR = \"dataset/val2017/\"\n",
    "test_images = os.listdir(DATASET_DIR)\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "target_layers = [model.backbone]\n",
    "\n",
    "Gradcam = GradCAM(model,\n",
    "                    [model.backbone.body.layer4[2].conv3])\n",
    "\n",
    "for image in test_images:\n",
    "    image_name = image\n",
    "    image = np.array(Image.open(DATASET_DIR + image))\n",
    "    original_image = image.copy()\n",
    "    image_float_np = np.float32(image) / 255\n",
    "    \n",
    "    input_tensor = transform(image_float_np)\n",
    "    input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "    # Run the model and display the detections\n",
    "    boxes, classes, labels, indices = predict(input_tensor, model, 0.9)\n",
    "    targets = [FasterRCNNBoxScoreTarget(labels=labels, bounding_boxes=boxes)]\n",
    "\n",
    "    image_with_predictions = draw_boxes(boxes, labels, classes, image)\n",
    "\n",
    "    # Computing GradCAM\n",
    "    grayscale_gradcam = Gradcam(input_tensor=input_tensor, targets=targets)[0, :]\n",
    "    gradcam = show_cam_on_image(image_float_np, grayscale_gradcam, use_rgb=True)\n",
    "    if len(boxes) == 0:\n",
    "        continue\n",
    "    renormalized_gradcam = renormalize_cam_in_bounding_boxes(boxes, image_float_np, grayscale_gradcam)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(10, 5))\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title(\"Input Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    # Image with predicted bounding boxes\n",
    "    axes[1].imshow(image_with_predictions)\n",
    "    axes[1].set_title(\"Predicted Boxes\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    # GradCAM heatmap\n",
    "    axes[2].imshow(gradcam)\n",
    "    axes[2].set_title(\"GradCAM Heatmap\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    # GradCAM heatmap renormalized in bounding boxes\n",
    "    axes[3].imshow(renormalized_gradcam)\n",
    "    axes[3].set_title(\"GradCAM Heatmap\\nRenormalized in Bounding Boxes\")\n",
    "    axes[3].axis(\"off\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # Saving the images\n",
    "    output_dir = \"output/gradcam/val/\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    plt.savefig(output_dir + image_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EigenCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"dataset/val2017/\"\n",
    "test_images = os.listdir(DATASET_DIR)\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "target_layers = [model.backbone]\n",
    "\n",
    "Eigencam = EigenCAM(model,\n",
    "               target_layers, \n",
    "               reshape_transform=fasterrcnn_reshape_transform)\n",
    "\n",
    "for image in test_images:\n",
    "    image_name = image\n",
    "    image = np.array(Image.open(DATASET_DIR + image))\n",
    "    original_image = image.copy()\n",
    "    image_float_np = np.float32(image) / 255\n",
    "    \n",
    "    input_tensor = transform(image_float_np)\n",
    "    input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "    # Run the model and display the detections\n",
    "    boxes, classes, labels, indices = predict(input_tensor, model, 0.9)\n",
    "    targets = [FasterRCNNBoxScoreTarget(labels=labels, bounding_boxes=boxes)]\n",
    "\n",
    "    image_with_predictions = draw_boxes(boxes, labels, classes, image)\n",
    "\n",
    "    # Computing EigenCAM\n",
    "    grayscale_eigencam = Eigencam(input_tensor=input_tensor, targets=targets)[0, :]\n",
    "    eigencam_image = show_cam_on_image(image_float_np, grayscale_eigencam, use_rgb=True)\n",
    "    if len(boxes) == 0:\n",
    "        continue\n",
    "    renormalized_eigencam_image = renormalize_cam_in_bounding_boxes(boxes, image_float_np, grayscale_eigencam)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(10, 5))\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title(\"Input Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    # Image with predicted bounding boxes\n",
    "    axes[1].imshow(image_with_predictions)\n",
    "    axes[1].set_title(\"Predicted Boxes\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    # EigenCAM heatmap\n",
    "    axes[2].imshow(eigencam_image)\n",
    "    axes[2].set_title(\"EigenCAM Heatmap\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    # EigenCAM heatmap renormalized in bounding boxes\n",
    "    axes[3].imshow(renormalized_eigencam_image)\n",
    "    axes[3].set_title(\"EigenCAM Heatmap\\nRenormalized in Bounding Boxes\")\n",
    "    axes[3].axis(\"off\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # Saving the images\n",
    "    output_dir = \"output/eigencam/val/\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    plt.savefig(output_dir + image_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AblationCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"dataset/val2017/\"\n",
    "test_images = os.listdir(DATASET_DIR)\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "target_layers = [model.backbone]\n",
    "\n",
    "Ablationcam = AblationCAM(model,\n",
    "                  target_layers, \n",
    "                  reshape_transform=fasterrcnn_reshape_transform,\n",
    "                  ablation_layer=AblationLayerFasterRCNN(),\n",
    "                  ratio_channels_to_ablate=0.01)\n",
    "\n",
    "for image in test_images:\n",
    "    image_name = image\n",
    "    image = np.array(Image.open(DATASET_DIR + image))\n",
    "    original_image = image.copy()\n",
    "    image_float_np = np.float32(image) / 255\n",
    "    \n",
    "    input_tensor = transform(image_float_np)\n",
    "    input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "    # Run the model and display the detections\n",
    "    boxes, classes, labels, indices = predict(input_tensor, model, 0.9)\n",
    "    targets = [FasterRCNNBoxScoreTarget(labels=labels, bounding_boxes=boxes)]\n",
    "\n",
    "    image_with_predictions = draw_boxes(boxes, labels, classes, image)\n",
    "\n",
    "    # Computing AblationCAM\n",
    "    grayscale_ablationcam = Ablationcam(input_tensor=input_tensor, targets=targets)[0, :]\n",
    "    ablationcam = show_cam_on_image(image_float_np, grayscale_ablationcam, use_rgb=True)\n",
    "    if len(boxes) == 0:\n",
    "        continue\n",
    "    renormalized_ablationcam = renormalize_cam_in_bounding_boxes(boxes, image_float_np, grayscale_ablationcam)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(10, 5))\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title(\"Input Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    # Image with predicted bounding boxes\n",
    "    axes[1].imshow(image_with_predictions)\n",
    "    axes[1].set_title(\"Predicted Boxes\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    # AblationCAM heatmap\n",
    "    axes[2].imshow(ablationcam)\n",
    "    axes[2].set_title(\"AblationCAM Heatmap\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    # AblationCAM heatmap renormalized in bounding boxes\n",
    "    axes[3].imshow(renormalized_ablationcam)\n",
    "    axes[3].set_title(\"AblationCAM Heatmap\\nRenormalized in Bounding Boxes\")\n",
    "    axes[3].axis(\"off\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # Saving the images\n",
    "    output_dir = \"output/ablation/val/\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    plt.savefig(output_dir + image_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept Activation Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Feature Factorizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import DeepFeatureFactorization\n",
    "from pytorch_grad_cam.utils.image import show_factorization_on_image\n",
    "DATASET_DIR = \"dataset/val2017/\"\n",
    "test_images = os.listdir(DATASET_DIR)\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "dff = DeepFeatureFactorization(model=model, target_layer=model.backbone.body.layer4[-1].conv3, computation_on_concepts=model.roi_heads.box_roi_pool)\n",
    "\n",
    "\n",
    "for image in test_images:\n",
    "    image_name = image\n",
    "    image = np.array(Image.open(DATASET_DIR + image))\n",
    "    original_image = image.copy()\n",
    "    image_float_np = np.float32(image) / 255\n",
    "    \n",
    "    input_tensor = transform(image_float_np)\n",
    "    input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "    # Run the model and display the detections\n",
    "    boxes, classes, labels, indices = predict(input_tensor, model, 0.9)\n",
    "    \n",
    "    concepts, batch_explanations, concept_scores = dff(input_tensor, n_components=5)\n",
    "    visualization = show_factorization_on_image(image_float_np, \n",
    "                                                batch_explanations[0],\n",
    "                                                image_weight=0.3)\n",
    "    \n",
    "    if len(boxes) == 0:\n",
    "        continue\n",
    "\n",
    "    renormalized_dff = renormalize_cam_in_bounding_boxes(boxes, image_float_np, batch_explanations[0])\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(10, 5))\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title(\"Input Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "    \n",
    "    # Image with predicted bounding boxes\n",
    "    axes[1].imshow(image_with_predictions)\n",
    "    axes[1].set_title(\"Predicted Boxes\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    # Deep Feature Factorization heatmap\n",
    "    axes[2].imshow(visualization)\n",
    "    axes[2].set_title(\"Deep Feature Factorization Heatmap\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    # Deep Feature Factorization heatmap renormalized in bounding boxes\n",
    "    axes[3].imshow(renormalized_dff)\n",
    "    axes[3].set_title(\"Deep Feature Factorization Heatmap\\nRenormalized in Bounding Boxes\")\n",
    "    axes[3].axis(\"off\")\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # Saving the images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
