{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "import os\n",
    "from coco_labels import get_coco_labels\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "coco_labels = ['__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', \\\n",
    "              'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', \n",
    "              'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', \n",
    "              'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella',\n",
    "              'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n",
    "              'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
    "              'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass', 'cup', 'fork',\n",
    "              'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n",
    "              'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "              'potted plant', 'bed', 'N/A', 'dining table', 'N/A', 'N/A', 'toilet',\n",
    "              'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n",
    "              'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book', 'clock', 'vase',\n",
    "              'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
    "coco_names = coco_labels.copy()\n",
    "              \n",
    "def read_image(dataset_dir, image_name):\n",
    "    image = cv2.imread(dataset_dir + image_name)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "def plot_boxes(image, boxes, labels, scores, coco_labels):\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        box = [int(coord) for coord in box]\n",
    "        score = float(score)\n",
    "        cv2.rectangle(image, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "        # Convert label ID to corresponding class name\n",
    "        class_name = coco_labels[label]\n",
    "        if class_name == \"N/A\":\n",
    "            continue\n",
    "        text = \"{}: {:.2f}\".format(class_name, score)\n",
    "        cv2.putText(image, text, (box[0], box[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    return image\n",
    "\n",
    "def predict_boxes(model, image_tensor, confidence_threshold, iou_threshold):\n",
    "    # Forward pass through the model\n",
    "    output = model(image_tensor)\n",
    "\n",
    "    # Visualize predictions on the image\n",
    "    boxes, labels, scores = output[0]['boxes'], output[0]['labels'], output[0]['scores']\n",
    "\n",
    "    # Apply confidence thresholding\n",
    "    keep = scores > confidence_threshold\n",
    "    boxes, labels, scores = boxes[keep], labels[keep], scores[keep]\n",
    "\n",
    "    # Perform non-maximum suppression using NMS\n",
    "    keep_idxs = torchvision.ops.nms(boxes, scores, iou_threshold=iou_threshold)\n",
    "\n",
    "    # Apply NMS to retain the most confident detections\n",
    "    boxes, labels, scores = boxes[keep_idxs], labels[keep_idxs], scores[keep_idxs]\n",
    "\n",
    "    return boxes, labels, scores\n",
    "\n",
    "def predict(input_tensor, model, detection_threshold):\n",
    "    outputs = model(input_tensor)\n",
    "    pred_classes = [coco_names[i] for i in outputs[0]['labels'].cpu().numpy()]\n",
    "    pred_labels = outputs[0]['labels'].cpu().numpy()\n",
    "    pred_scores = outputs[0]['scores'].detach().cpu().numpy()\n",
    "    pred_bboxes = outputs[0]['boxes'].detach().cpu().numpy()\n",
    "    \n",
    "    boxes, classes, labels, indices = [], [], [], []\n",
    "    for index in range(len(pred_scores)):\n",
    "        if pred_scores[index] >= detection_threshold:\n",
    "            boxes.append(pred_bboxes[index].astype(np.int32))\n",
    "            classes.append(pred_classes[index])\n",
    "            labels.append(pred_labels[index])\n",
    "            indices.append(index)\n",
    "    boxes = np.int32(boxes)\n",
    "    return boxes, classes, labels, indices\n",
    "\n",
    "def draw_boxes(boxes, labels, classes, image):\n",
    "    for i, box in enumerate(boxes):\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (int(box[0]), int(box[1])),\n",
    "            (int(box[2]), int(box[3])),\n",
    "            (0, 255, 0), 2\n",
    "        )\n",
    "        cv2.putText(image, classes[i], (int(box[0]), int(box[1] - 5)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2,\n",
    "                    lineType=cv2.LINE_AA)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference & Evaluation (IoU & mAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `torchvision.models.detection` module provides several pre-trained models for object detection, segmentation, and person keypoint detection, as well as some training utilities. For Faster R-CNN, the following models are available:\n",
    "\n",
    "1. `fasterrcnn_resnet50_fpn`: This is a Faster R-CNN model with a ResNet-50 backbone and Feature Pyramid Network (FPN). It's pre-trained on the COCO train2017 dataset.\n",
    "\n",
    "2. `fasterrcnn_mobilenet_v3_large_fpn`: This is a Faster R-CNN model with a MobileNetV3-Large backbone and FPN. It's also pre-trained on the COCO train2017 dataset.\n",
    "\n",
    "3. `fasterrcnn_mobilenet_v3_large_320_fpn`: This is another Faster R-CNN model with a MobileNetV3-Large backbone and FPN, but designed for 320x320 input images. It's pre-trained on the COCO train2017 dataset.\n",
    "\n",
    "You can use these models like this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(images, model, transform, confidence_threshold, coco_labels, show=False, save=True, split='val'):\n",
    "\n",
    "    output_dir = \"output/\" + split + \"/confidence_threshold_\" + str(confidence_threshold) + \"/\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for sample in images:\n",
    "        image = np.array(Image.open(DATASET_DIR + sample))\n",
    "        image_float_np = np.float32(image) / 255\n",
    "        \n",
    "        input_tensor = transform(image_float_np)\n",
    "        input_tensor = input_tensor.unsqueeze(0)\n",
    "        \n",
    "        boxes, classes, labels, _ = predict(input_tensor, model, confidence_threshold)\n",
    "        image = draw_boxes(boxes, labels, classes, image)\n",
    "        \n",
    "        if show:\n",
    "            cv2.imshow(\"Image\", image)\n",
    "            cv2.waitKey(0)\n",
    "\n",
    "        if save:\n",
    "            cv2.imwrite(output_dir + sample, image)\n",
    "\n",
    "\n",
    "# Constants\n",
    "DATASET_DIR = \"dataset/val2017/\"\n",
    "CONFIDENCE_THRESHOLD = 0.9\n",
    "IOU_THRESHOLD = 0.5\n",
    "\n",
    "# Load the COCO dataset in \"dataset/test2017\" \n",
    "test_images = os.listdir(DATASET_DIR)\n",
    "\n",
    "# Load the pre-trained Faster R-CNN model trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "model.eval()\n",
    "\n",
    "# Define the transformation to be applied to images\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "# Applying inference on the images\n",
    "inference(test_images, model, transform, CONFIDENCE_THRESHOLD, coco_labels, show=False, save=True, split='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation visualization is a technique used in deep learning to understand and analyze the behavior of neural networks. It involves visualizing the activations or outputs of individual neurons or layers within a neural network when given a particular input.\n",
    "\n",
    "The main purpose of activation visualization is to gain insights into how the network is processing and representing the input data. By visualizing the activations, we can identify which parts of the input are being emphasized or ignored by the network. This can help us understand the network's decision-making process and potentially identify any issues or biases in the model.\n",
    "\n",
    "In general, the process involves the following steps:\n",
    "\n",
    "1. Load the pre-trained model: Activation visualization is typically performed on pre-trained models. The first step is to load the model architecture and weights.\n",
    "\n",
    "2. Prepare the input data: Depending on the task, you need to prepare the input data that the model expects. This could involve preprocessing, resizing, or normalizing the input images.\n",
    "\n",
    "3. Forward pass: Pass the input data through the model to obtain the activations. This involves feeding the input data through the layers of the model and collecting the activations at the desired layer(s).\n",
    "\n",
    "4. Visualize the activations: Once you have obtained the activations, you can visualize them using various techniques such as heatmaps, feature maps, or activation histograms. These visualizations can provide insights into the learned representations and patterns within the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_visualization(images, model, transform, show=False, save=True, split='val'):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    output_dir = \"output/activation_visualization/\" + split + \"/\"\n",
    "\n",
    "    for sample in images:\n",
    "\n",
    "        # Read the image from disk using the image_name\n",
    "        image_name = sample\n",
    "        image = read_image(DATASET_DIR, image_name)\n",
    "        image_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        output = model(image_tensor)\n",
    "\n",
    "        # Get the feature maps from the model\n",
    "        feature_maps = model.backbone.body(image_tensor)\n",
    "        first_layer_activations = feature_maps['0'][0]\n",
    "        last_layer_activations = feature_maps['3'][0]\n",
    "        print(\"first_layer_activations: \", first_layer_activations.shape)\n",
    "        print(\"last_layer_activations: \", last_layer_activations.shape)\n",
    "\n",
    "        # Nomralize the activations\n",
    "        first_layer_activations = (first_layer_activations - first_layer_activations.min()) / (first_layer_activations.max() - first_layer_activations.min())\n",
    "        last_layer_activations = (last_layer_activations - last_layer_activations.min()) / (last_layer_activations.max() - last_layer_activations.min())\n",
    "        \n",
    "        # Visualize the activations\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(10, 5))\n",
    "\n",
    "        # First layer activations for only the first 3 filters\n",
    "        axes[0, 0].imshow(torchvision.transforms.ToPILImage()(first_layer_activations[0]), cmap=\"gray\")\n",
    "        axes[0, 0].set_title(\"First Layer Activations\\nFilter 1\")\n",
    "        axes[0, 0].axis(\"off\")\n",
    "\n",
    "        axes[0, 1].imshow(torchvision.transforms.ToPILImage()(first_layer_activations[1]), cmap=\"gray\")\n",
    "        axes[0, 1].set_title(\"First Layer Activations\\nFilter 2\")\n",
    "        axes[0, 1].axis(\"off\")\n",
    "\n",
    "        axes[0, 2].imshow(torchvision.transforms.ToPILImage()(first_layer_activations[2]), cmap=\"gray\")\n",
    "        axes[0, 2].set_title(\"First Layer Activations\\nFilter 3\")\n",
    "        axes[0, 2].axis(\"off\")\n",
    "\n",
    "        # Last layer activations for only the first 3 filters\n",
    "        axes[1, 0].imshow(torchvision.transforms.ToPILImage()(last_layer_activations[0]), cmap=\"gray\")\n",
    "        axes[1, 0].set_title(\"Last Layer Activations\\nFilter 1\")\n",
    "        axes[1, 0].axis(\"off\")\n",
    "\n",
    "        axes[1, 1].imshow(torchvision.transforms.ToPILImage()(last_layer_activations[1]), cmap=\"gray\")\n",
    "        axes[1, 1].set_title(\"Last Layer Activations\\nFilter 2\")\n",
    "        axes[1, 1].axis(\"off\")\n",
    "\n",
    "        axes[1, 2].imshow(torchvision.transforms.ToPILImage()(last_layer_activations[2]), cmap=\"gray\")\n",
    "        axes[1, 2].set_title(\"Last Layer Activations\\nFilter 3\")\n",
    "        axes[1, 2].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            plt.savefig(output_dir + image_name)\n",
    "\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "# Constants\n",
    "DATASET_DIR = \"dataset/val2017/\"\n",
    "print(\"Loaded {} labels from COCO dataset\".format(len(coco_labels)))\n",
    "\n",
    "# Load the pre-trained Faster R-CNN model trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Define the transformation to be applied to images\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "# Load the COCO dataset in \"dataset/test2017\"\n",
    "test_images = os.listdir(DATASET_DIR)\n",
    "test_images = test_images[:5]\n",
    "\n",
    "# Apply activation visualization\n",
    "activation_visualization(test_images, model, transform, show=True, save=True, split='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grad CAM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRAD CAM (Gradient-weighted Class Activation Mapping) is a technique used to visualize and understand the regions of an image that are important for a deep learning model's prediction. It helps in interpreting the decision-making process of the model by highlighting the areas that contribute the most to the final prediction.\n",
    "\n",
    "The main purpose of using GRAD CAM is to gain insights into how the model is focusing on specific regions of the input image to make its predictions. By visualizing the important regions, we can understand which parts of the image are influencing the model's decision and identify any biases or issues in the model.\n",
    "\n",
    "To use GRAD CAM, you need to follow these steps:\n",
    "\n",
    "1. Load the pre-trained model: In the provided code, a pre-trained Faster R-CNN model trained on the COCO dataset is loaded using `torchvision.models.detection.fasterrcnn_resnet50_fpn()`. This model is used for object detection.\n",
    "\n",
    "2. Prepare the input data: The input images are loaded from the dataset directory and transformed using `torchvision.transforms.Compose()`.\n",
    "\n",
    "3. Define the Grad CAM function: The `grad_cam()` function takes the images, model, transform, confidence threshold, IOU threshold, COCO labels, show flag, save flag, and split as input parameters. It sets the model to evaluation mode and defines the output directory for saving the results.\n",
    "\n",
    "4. Iterate over the images: For each image, the function reads the image from disk, creates a copy of the original image, and transforms it into a tensor. The tensor is set to require gradients for backpropagation.\n",
    "\n",
    "5. Predict boxes: The model predicts the bounding boxes, labels, and scores for the image using the `predict_boxes()` function.\n",
    "\n",
    "6. Plot boxes: The function plots the predicted boxes on the image using the `plot_boxes()` function.\n",
    "\n",
    "7. Backpropagate gradients: The gradients are backpropagated based on the predicted class scores. The model is zeroed out and the gradients are computed for each score.\n",
    "\n",
    "8. Get activations and gradients: The activations from the last convolutional layer and the gradients of the output with respect to the last convolutional layer are obtained.\n",
    "\n",
    "9. Compute Grad CAM heatmap: The Grad CAM heatmap is computed by multiplying the weights (mean values of gradients) with the last activation maps.\n",
    "\n",
    "10. Normalize and resize Grad CAM heatmap: The Grad CAM heatmap is normalized and resized to match the input image size.\n",
    "\n",
    "11. Visualize the results: The original image, predicted boxes, and Grad CAM heatmap are visualized using matplotlib.\n",
    "\n",
    "12. Save or show the results: The results can be saved to the output directory or shown using the `save` and `show` flags.\n",
    "\n",
    "By following these steps, you can use GRAD CAM to visualize and interpret the important regions in an image that contribute to the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_cam(images, model, transform, confidence_threshold, iou_threshold, coco_labels, show = False, save = True, split='val'):\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    output_dir = \"output/gradcam/\" + split + \"/confidence_threshold_\" + str(confidence_threshold) + \"_iou_threshold_\" + str(iou_threshold) + \"/\"\n",
    "\n",
    "    for sample in images:\n",
    "\n",
    "        # Read the image from disk using the image_name\n",
    "        image_name = sample\n",
    "        image = read_image(DATASET_DIR, sample)\n",
    "        original_image = image.copy()\n",
    "        image_tensor = transform(image).unsqueeze(0)\n",
    "        image_tensor.requires_grad = True  \n",
    "\n",
    "        boxes, labels, scores = predict_boxes(model, image_tensor, confidence_threshold, iou_threshold)\n",
    "\n",
    "        if len(boxes) == 0:\n",
    "            continue\n",
    "        image = plot_boxes(image, boxes, labels, scores, coco_labels)\n",
    "\n",
    "        # Backpropagate the gradients based on the predicted class score\n",
    "        model.zero_grad()\n",
    "        for score in scores:\n",
    "            score.backward(retain_graph=True)\n",
    "        \n",
    "        # Get the activations from the last convolutional layer\n",
    "        last_activation = model.backbone.body(image_tensor)['3'][0].detach().numpy()\n",
    "\n",
    "        # Get the gradients of the output with respect to the last convolutional layer\n",
    "        grads = model.backbone.body.layer4[2].conv3.weight.grad.detach().numpy()\n",
    "        grads = grads.squeeze(axis=(2, 3))\n",
    "\n",
    "        # Get the mean value of the gradients for every feature map\n",
    "        weights = np.mean(grads, axis=(1))\n",
    "\n",
    "        grad_cam = np.zeros(last_activation.shape[1:], dtype=np.float32)\n",
    "        for i, w in enumerate(weights):\n",
    "            grad_cam += w * last_activation[i, :, :]\n",
    "\n",
    "\n",
    "        # Normalize Grad-CAM heatmap\n",
    "        grad_cam = np.maximum(grad_cam, 0)\n",
    "        grad_cam = (grad_cam - grad_cam.min()) / (grad_cam.max() - grad_cam.min())\n",
    "\n",
    "        # Resize Grad-CAM heatmap to match the input image size\n",
    "        grad_cam = cv2.resize(grad_cam, (image.shape[1], image.shape[0]))\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(10, 5))\n",
    "\n",
    "        # Input image\n",
    "        axes[0].imshow(original_image)\n",
    "        axes[0].set_title(\"Input Image\")\n",
    "        axes[0].axis(\"off\")\n",
    "\n",
    "        # Predicted box within the image\n",
    "        axes[1].imshow(image)\n",
    "        axes[1].set_title(\"Predicted Boxes\")\n",
    "        axes[1].axis(\"off\")\n",
    "\n",
    "        # Grad-CAM heatmap\n",
    "        axes[2].imshow(grad_cam, cmap='jet', alpha=0.5)\n",
    "        axes[2].imshow(original_image, alpha=0.5)\n",
    "        axes[2].set_title(\"Grad-CAM Heatmap\")\n",
    "        axes[2].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            plt.savefig(output_dir + image_name)\n",
    "\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "# Constants\n",
    "DATASET_DIR = \"dataset/val2017/\"\n",
    "CONFIDENCE_THRESHOLD = 0.9\n",
    "IOU_THRESHOLD = 0.5\n",
    "coco_labels = get_coco_labels()\n",
    "print(\"Loaded {} labels from COCO dataset\".format(len(coco_labels)))\n",
    "\n",
    "# Load the pre-trained Faster R-CNN model trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "model.eval()\n",
    "\n",
    "# Define the transformation to be applied to images\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "# Load the COCO dataset in \"dataset/test2017\"\n",
    "test_images = os.listdir(DATASET_DIR)\n",
    "\n",
    "# Apply Grad CAM\n",
    "grad_cam(test_images, model, transform, CONFIDENCE_THRESHOLD, IOU_THRESHOLD, coco_labels, show=True, save=True, split='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EigenCAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigen-CAM is a method to generate class activation maps (CAMs) for convolutional neural networks (CNNs) using principal component analysis. The main steps to compute Eigen-CAM are:\n",
    "\n",
    "1- Forward pass an image through a pretrained CNN model up to the last convolutional layer to obtain the feature maps (activations).\n",
    "\n",
    "2- Reshape the feature maps into a matrix and perform singular value decomposition (SVD) on it to obtain the principal components.\n",
    "\n",
    "3- Take the first principal component which corresponds to the direction of maximum variance in the data.\n",
    "\n",
    "4- Reshape the principal component to the dimensions of the feature map to obtain the Eigen-CAM.\n",
    "\n",
    "5- The Eigen-CAM highlights the regions in the image that contribute most to the CNN's output for a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import os\n",
    "from pytorch_grad_cam.ablation_layer import AblationLayerFasterRCNN\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from pytorch_grad_cam import GradCAM, AblationCAM, EigenCAM, ScoreCAM, GradCAMPlusPlus\n",
    "from pytorch_grad_cam.utils.model_targets import FasterRCNNBoxScoreTarget\n",
    "from pytorch_grad_cam.utils.reshape_transforms import fasterrcnn_reshape_transform\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, scale_cam_image\n",
    "\n",
    "\n",
    "def fasterrcnn_reshape_transform(x):\n",
    "    target_size = x['pool'].size()[-2 : ]\n",
    "    activations = []\n",
    "    for key, value in x.items():\n",
    "        activations.append(torch.nn.functional.interpolate(torch.abs(value), target_size, mode='bilinear'))\n",
    "    activations = torch.cat(activations, axis=1)\n",
    "    return activations\n",
    "\n",
    "class FasterRCNNBoxScoreTarget:\n",
    "    \"\"\" For every original detected bounding box specified in \"bounding boxes\",\n",
    "        assign a score on how the current bounding boxes match it,\n",
    "            1. In IOU\n",
    "            2. In the classification score.\n",
    "        If there is not a large enough overlap, or the category changed,\n",
    "        assign a score of 0.\n",
    "\n",
    "        The total score is the sum of all the box scores.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, labels, bounding_boxes, iou_threshold=0.5):\n",
    "        self.labels = labels\n",
    "        self.bounding_boxes = bounding_boxes\n",
    "        self.iou_threshold = iou_threshold\n",
    "\n",
    "    def __call__(self, model_outputs):\n",
    "        output = torch.Tensor([0])\n",
    "\n",
    "        if len(model_outputs[\"boxes\"]) == 0:\n",
    "            return output\n",
    "\n",
    "        for box, label in zip(self.bounding_boxes, self.labels):\n",
    "            box = torch.Tensor(box[None, :])\n",
    "\n",
    "            ious = torchvision.ops.box_iou(box, model_outputs[\"boxes\"])\n",
    "            index = ious.argmax()\n",
    "            if ious[0, index] > self.iou_threshold and model_outputs[\"labels\"][index] == label:\n",
    "                score = ious[0, index] + model_outputs[\"scores\"][index]\n",
    "                output = output + score\n",
    "        return output\n",
    "    \n",
    "def renormalize_cam_in_bounding_boxes(boxes, image_float_np, grayscale_cam):\n",
    "    \"\"\"Normalize the CAM to be in the range [0, 1] \n",
    "    inside every bounding boxes, and zero outside of the bounding boxes. \"\"\"\n",
    "    renormalized_cam = np.zeros(grayscale_cam.shape, dtype=np.float32)\n",
    "    images = []\n",
    "    for x1, y1, x2, y2 in boxes:\n",
    "        img = renormalized_cam * 0\n",
    "        img[y1:y2, x1:x2] = scale_cam_image(grayscale_cam[y1:y2, x1:x2].copy())    \n",
    "        images.append(img)\n",
    "    \n",
    "    renormalized_cam = np.max(np.float32(images), axis = 0)\n",
    "    renormalized_cam = scale_cam_image(renormalized_cam)\n",
    "    eigencam_image_renormalized = show_cam_on_image(image_float_np, renormalized_cam, use_rgb=True)\n",
    "    image_with_bounding_boxes = draw_boxes(boxes, labels, classes, eigencam_image_renormalized)\n",
    "    return image_with_bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"dataset/val2017/\"\n",
    "test_images = os.listdir(DATASET_DIR)\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "target_layers = [model.backbone]\n",
    "\n",
    "Eigencam = EigenCAM(model,\n",
    "               target_layers, \n",
    "               reshape_transform=fasterrcnn_reshape_transform)\n",
    "\n",
    "for image in test_images:\n",
    "    image_name = image\n",
    "    image = np.array(Image.open(DATASET_DIR + image))\n",
    "    original_image = image.copy()\n",
    "    image_float_np = np.float32(image) / 255\n",
    "    \n",
    "    input_tensor = transform(image_float_np)\n",
    "    input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "    # Run the model and display the detections\n",
    "    boxes, classes, labels, indices = predict(input_tensor, model, 0.9)\n",
    "    targets = [FasterRCNNBoxScoreTarget(labels=labels, bounding_boxes=boxes)]\n",
    "\n",
    "    image_with_predictions = draw_boxes(boxes, labels, classes, image)\n",
    "\n",
    "    # Computing EigenCAM\n",
    "    grayscale_eigencam = Eigencam(input_tensor=input_tensor, targets=targets)[0, :]\n",
    "    eigencam_image = show_cam_on_image(image_float_np, grayscale_eigencam, use_rgb=True)\n",
    "    if len(boxes) == 0:\n",
    "        continue\n",
    "    renormalized_eigencam_image = renormalize_cam_in_bounding_boxes(boxes, image_float_np, grayscale_eigencam)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(10, 5))\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title(\"Input Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    # Image with predicted bounding boxes\n",
    "    axes[1].imshow(image_with_predictions)\n",
    "    axes[1].set_title(\"Predicted Boxes\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    # EigenCAM heatmap\n",
    "    axes[2].imshow(eigencam_image)\n",
    "    axes[2].set_title(\"EigenCAM Heatmap\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    # EigenCAM heatmap renormalized in bounding boxes\n",
    "    axes[3].imshow(renormalized_eigencam_image)\n",
    "    axes[3].set_title(\"EigenCAM Heatmap\\nRenormalized in Bounding Boxes\")\n",
    "    axes[3].axis(\"off\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # Saving the images\n",
    "    output_dir = \"output/eigencam/val/\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    plt.savefig(output_dir + image_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AblationCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"dataset/val2017/\"\n",
    "test_images = os.listdir(DATASET_DIR)\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "target_layers = [model.backbone]\n",
    "\n",
    "Ablationcam = AblationCAM(model,\n",
    "                  target_layers, \n",
    "                  reshape_transform=fasterrcnn_reshape_transform,\n",
    "                  ablation_layer=AblationLayerFasterRCNN(),\n",
    "                  ratio_channels_to_ablate=0.01)\n",
    "\n",
    "for image in test_images:\n",
    "    image_name = image\n",
    "    image = np.array(Image.open(DATASET_DIR + image))\n",
    "    original_image = image.copy()\n",
    "    image_float_np = np.float32(image) / 255\n",
    "    \n",
    "    input_tensor = transform(image_float_np)\n",
    "    input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "    # Run the model and display the detections\n",
    "    boxes, classes, labels, indices = predict(input_tensor, model, 0.9)\n",
    "    targets = [FasterRCNNBoxScoreTarget(labels=labels, bounding_boxes=boxes)]\n",
    "\n",
    "    image_with_predictions = draw_boxes(boxes, labels, classes, image)\n",
    "\n",
    "    # Computing AblationCAM\n",
    "    grayscale_ablationcam = Ablationcam(input_tensor=input_tensor, targets=targets)[0, :]\n",
    "    ablationcam = show_cam_on_image(image_float_np, grayscale_ablationcam, use_rgb=True)\n",
    "    if len(boxes) == 0:\n",
    "        continue\n",
    "    renormalized_ablationcam = renormalize_cam_in_bounding_boxes(boxes, image_float_np, grayscale_ablationcam)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(10, 5))\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title(\"Input Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    # Image with predicted bounding boxes\n",
    "    axes[1].imshow(image_with_predictions)\n",
    "    axes[1].set_title(\"Predicted Boxes\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    # AblationCAM heatmap\n",
    "    axes[2].imshow(ablationcam)\n",
    "    axes[2].set_title(\"AblationCAM Heatmap\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    # AblationCAM heatmap renormalized in bounding boxes\n",
    "    axes[3].imshow(renormalized_ablationcam)\n",
    "    axes[3].set_title(\"AblationCAM Heatmap\\nRenormalized in Bounding Boxes\")\n",
    "    axes[3].axis(\"off\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # Saving the images\n",
    "    output_dir = \"output/ablation/val/\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    plt.savefig(output_dir + image_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept Activation Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Feature Factorizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
