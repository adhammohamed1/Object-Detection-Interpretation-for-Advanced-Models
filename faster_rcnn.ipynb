{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster R-CNN: Explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "from pytorch_grad_cam.ablation_layer import AblationLayerFasterRCNN\n",
    "from pytorch_grad_cam import GradCAM, AblationCAM, EigenCAM, ScoreCAM, GradCAMPlusPlus, DeepFeatureFactorization\n",
    "from pytorch_grad_cam.utils.model_targets import FasterRCNNBoxScoreTarget\n",
    "from pytorch_grad_cam.utils.reshape_transforms import fasterrcnn_reshape_transform\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, scale_cam_image, show_factorization_on_image\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "# Defining COCO categories\n",
    "coco_labels = ['__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
    "              'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', \n",
    "              'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', \n",
    "              'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella',\n",
    "              'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n",
    "              'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
    "              'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass', 'cup', 'fork',\n",
    "              'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n",
    "              'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "              'potted plant', 'bed', 'N/A', 'dining table', 'N/A', 'N/A', 'toilet',\n",
    "              'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n",
    "              'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book', 'clock', 'vase',\n",
    "              'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
    "              \n",
    "def read_image(dataset_dir, image_name) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Read and preprocess an image from the dataset directory.\n",
    "\n",
    "    Args:\n",
    "        dataset_dir (str): The directory path of the dataset.\n",
    "        image_name (str): The name of the image file.\n",
    "\n",
    "    Returns:\n",
    "        image (numpy.ndarray): The preprocessed image as a NumPy array.\n",
    "\n",
    "    \"\"\"\n",
    "    # Read the image using OpenCV\n",
    "    image = cv2.imread(dataset_dir + image_name)\n",
    "\n",
    "    # Convert the image from BGR to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "\n",
    "def predict(input_tensor, model, detection_threshold) -> tuple:\n",
    "    \"\"\"\n",
    "    Perform object detection on an input tensor using a Faster R-CNN model.\n",
    "\n",
    "    Args:\n",
    "        input_tensor (torch.Tensor): The input tensor to perform object detection on.\n",
    "        model (torch.nn.Module): The Faster R-CNN model.\n",
    "        detection_threshold (float): The minimum confidence score threshold for object detection.\n",
    "\n",
    "    Returns:\n",
    "        boxes (numpy.ndarray): An array of bounding boxes for the detected objects.\n",
    "        classes (list): A list of class labels for the detected objects.\n",
    "        labels (numpy.ndarray): An array of class labels for the detected objects.\n",
    "        indices (list): A list of indices corresponding to the detected objects.\n",
    "    \"\"\"\n",
    "    # Perform object detection using the model\n",
    "    outputs = model(input_tensor)\n",
    "\n",
    "    # Extract the predicted classes, labels, scores, and bounding boxes\n",
    "    pred_classes = [coco_labels[i] for i in outputs[0]['labels'].cpu().numpy()]\n",
    "    pred_labels = outputs[0]['labels'].cpu().numpy()\n",
    "    pred_scores = outputs[0]['scores'].detach().cpu().numpy()\n",
    "    pred_bboxes = outputs[0]['boxes'].detach().cpu().numpy()\n",
    "\n",
    "    # Initialize empty lists for storing the filtered results\n",
    "    boxes, classes, labels, indices = [], [], [], []\n",
    "\n",
    "    # Filter the results based on the detection threshold\n",
    "    for index in range(len(pred_scores)):\n",
    "        if pred_scores[index] >= detection_threshold:\n",
    "            boxes.append(pred_bboxes[index].astype(np.int32))\n",
    "            classes.append(pred_classes[index])\n",
    "            labels.append(pred_labels[index])\n",
    "            indices.append(index)\n",
    "\n",
    "    # Convert the boxes to numpy array\n",
    "    boxes = np.int32(boxes)\n",
    "\n",
    "    return boxes, classes, labels, indices\n",
    "\n",
    "\n",
    "def predict_with_scores(input_tensor, model, detection_threshold) -> tuple:\n",
    "    \"\"\"\n",
    "    Perform object detection on an input tensor using a Faster R-CNN model and return the bounding boxes,\n",
    "    class labels, labels, indices, and scores for the detected objects.\n",
    "\n",
    "    Args:\n",
    "        input_tensor (torch.Tensor): The input tensor to perform object detection on.\n",
    "        model (torch.nn.Module): The Faster R-CNN model.\n",
    "        detection_threshold (float): The minimum confidence score threshold for object detection.\n",
    "\n",
    "    Returns:\n",
    "        boxes (numpy.ndarray): An array of bounding boxes for the detected objects.\n",
    "        classes (list): A list of class labels for the detected objects.\n",
    "        labels (numpy.ndarray): An array of class labels for the detected objects.\n",
    "        indices (list): A list of indices corresponding to the detected objects.\n",
    "        scores (numpy.ndarray): An array of confidence scores for the detected objects.\n",
    "    \"\"\"\n",
    "    # Perform object detection using the model\n",
    "    outputs = model(input_tensor)\n",
    "\n",
    "    # Extract the predicted classes, labels, scores, and bounding boxes\n",
    "    pred_classes = [coco_labels[i] for i in outputs[0]['labels'].cpu().numpy()]\n",
    "    pred_labels = outputs[0]['labels'].cpu().numpy()\n",
    "    pred_scores = outputs[0]['scores'].detach().cpu().numpy()\n",
    "    pred_bboxes = outputs[0]['boxes'].detach().cpu().numpy()\n",
    "\n",
    "    # Initialize empty lists for storing the filtered results\n",
    "    boxes, classes, labels, indices, scores = [], [], [], [], []\n",
    "\n",
    "    # Filter the results based on the detection threshold\n",
    "    for index in range(len(pred_scores)):\n",
    "        if pred_scores[index] >= detection_threshold:\n",
    "            boxes.append(pred_bboxes[index].astype(np.int32))\n",
    "            classes.append(pred_classes[index])\n",
    "            labels.append(pred_labels[index])\n",
    "            indices.append(index)\n",
    "            scores.append(pred_scores[index])\n",
    "\n",
    "    # Convert the boxes to numpy array\n",
    "    boxes = np.int32(boxes)\n",
    "\n",
    "    return boxes, classes, labels, indices, scores\n",
    "\n",
    "\n",
    "def draw_boxes(boxes, labels, classes, image) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Draw bounding boxes and class labels on the input image.\n",
    "\n",
    "    Args:\n",
    "        boxes (numpy.ndarray): An array of bounding boxes for the detected objects.\n",
    "        labels (numpy.ndarray): An array of class labels for the detected objects.\n",
    "        classes (list): A list of class labels for the detected objects.\n",
    "        image (numpy.ndarray): The input image.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The image with bounding boxes and class labels drawn on it.\n",
    "    \"\"\"\n",
    "    for i, box in enumerate(boxes):\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (int(box[0]), int(box[1])),\n",
    "            (int(box[2]), int(box[3])),\n",
    "            (0, 255, 0), 2\n",
    "        )\n",
    "        cv2.putText(image, classes[i], (int(box[0]), int(box[1] - 5)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2,\n",
    "                    lineType=cv2.LINE_AA)\n",
    "    return image\n",
    "\n",
    "def fasterrcnn_reshape_transform(x):\n",
    "    \"\"\"\n",
    "    Reshape the output (activations) of a Faster R-CNN model to the shape of the input image.\n",
    "\n",
    "    Args:\n",
    "        x (dict): The output (activations) of a Faster R-CNN model.\n",
    "        \n",
    "    Returns:\n",
    "        dict: The reshaped output (activations) of a Faster R-CNN model.\n",
    "    \"\"\"\n",
    "    # Specify the target size (last two dimensions) of the reshaped output (height, width)\n",
    "    target_size = x['pool'].size()[-2 : ]\n",
    "    # Initialize an empty list for storing the activations\n",
    "    activations = []\n",
    "    # Iterate over the activations\n",
    "    for key, value in x.items():\n",
    "        # Resize the activation to the target size using bilinear interpolation\n",
    "        activations.append(torch.nn.functional.interpolate(torch.abs(value), target_size, mode='bilinear'))\n",
    "    # Concatenate the activations along the channel dimension\n",
    "    activations = torch.cat(activations, axis=1)\n",
    "    return activations\n",
    "\n",
    "class FasterRCNNBoxScoreTarget:\n",
    "    \"\"\" For every original detected bounding box specified in \"bounding boxes\",\n",
    "        assign a score on how the current bounding boxes match it,\n",
    "            1. In IOU\n",
    "            2. In the classification score.\n",
    "        If there is not a large enough overlap, or the category changed,\n",
    "        assign a score of 0.\n",
    "\n",
    "        The total score is the sum of all the box scores.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, labels, bounding_boxes, iou_threshold=0.5):\n",
    "        self.labels = labels\n",
    "        self.bounding_boxes = bounding_boxes\n",
    "        self.iou_threshold = iou_threshold\n",
    "\n",
    "    def __call__(self, model_outputs):\n",
    "        output = torch.Tensor([0])\n",
    "\n",
    "        if len(model_outputs[\"boxes\"]) == 0:\n",
    "            return output\n",
    "\n",
    "        for box, label in zip(self.bounding_boxes, self.labels):\n",
    "            box = torch.Tensor(box[None, :])\n",
    "\n",
    "            ious = torchvision.ops.box_iou(box, model_outputs[\"boxes\"])\n",
    "            index = ious.argmax()\n",
    "            if ious[0, index] > self.iou_threshold and model_outputs[\"labels\"][index] == label:\n",
    "                score = ious[0, index] + model_outputs[\"scores\"][index]\n",
    "                output = output + score\n",
    "        return output\n",
    "    \n",
    "def renormalize_cam_in_bounding_boxes(boxes, image_float_np, grayscale_cam):\n",
    "    \"\"\"Normalize the CAM to be in the range [0, 1] \n",
    "    inside every bounding boxes, and zero outside of the bounding boxes. \"\"\"\n",
    "    renormalized_cam = np.zeros(grayscale_cam.shape, dtype=np.float32)\n",
    "    images = []\n",
    "    for x1, y1, x2, y2 in boxes:\n",
    "        img = renormalized_cam * 0\n",
    "        img[y1:y2, x1:x2] = scale_cam_image(grayscale_cam[y1:y2, x1:x2].copy())    \n",
    "        images.append(img)\n",
    "    \n",
    "    renormalized_cam = np.max(np.float32(images), axis = 0)\n",
    "    renormalized_cam = scale_cam_image(renormalized_cam)\n",
    "    eigencam_image_renormalized = show_cam_on_image(image_float_np, renormalized_cam, use_rgb=True)\n",
    "    image_with_bounding_boxes = draw_boxes(boxes, labels, classes, eigencam_image_renormalized)\n",
    "    return image_with_bounding_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference & Evaluation (IoU & mAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `torchvision.models.detection` module provides several pre-trained models for object detection, segmentation, and person keypoint detection, as well as some training utilities. For Faster R-CNN, the following models are available:\n",
    "\n",
    "1. `fasterrcnn_resnet50_fpn`: This is a Faster R-CNN model with a ResNet-50 backbone and Feature Pyramid Network (FPN). It's pre-trained on the COCO train2017 dataset.\n",
    "\n",
    "2. `fasterrcnn_mobilenet_v3_large_fpn`: This is a Faster R-CNN model with a MobileNetV3-Large backbone and FPN. It's also pre-trained on the COCO train2017 dataset.\n",
    "\n",
    "3. `fasterrcnn_mobilenet_v3_large_320_fpn`: This is another Faster R-CNN model with a MobileNetV3-Large backbone and FPN, but designed for 320x320 input images. It's pre-trained on the COCO train2017 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining constants\n",
    "DATASET_DIR = \"dataset/val2017/\"\n",
    "CONFIDENCE_THRESHOLD = 0.9\n",
    "SAVE = False\n",
    "\n",
    "# Loading the COCO dataset in \"dataset/test2017\" \n",
    "test_images = os.listdir(DATASET_DIR)\n",
    "\n",
    "# Loading annotations\n",
    "coco_gt = COCO('dataset/instances_val2017.json')\n",
    "\n",
    "# Preparing the image info dictionary & filename to id dictionary\n",
    "image_ids = coco_gt.getImgIds()\n",
    "image_info = coco_gt.loadImgs(image_ids)\n",
    "filename_to_id = {img['file_name']: img['id'] for img in image_info}\n",
    "\n",
    "# Loading the pre-trained Faster R-CNN model trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "model.eval()\n",
    "\n",
    "# Defining the transformation to be applied to images\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "split = \"val\"\n",
    "\n",
    "# Creating the output directory\n",
    "output_dir = \"output (Faster_RCNN)/\" + split + \"/\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Initializing empty lists for storing the predictions\n",
    "predictions = []\n",
    "\n",
    "# Iterating over the images\n",
    "for sample in test_images:\n",
    "\n",
    "    # Reading the image\n",
    "    img_id = filename_to_id[sample]\n",
    "    image = np.array(Image.open(DATASET_DIR + sample).convert('RGB'))\n",
    "    input_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "    # Forward pass through the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)  \n",
    "\n",
    "    # Parse outputs\n",
    "    pred_boxes = outputs[0]['boxes'].data.cpu().numpy()\n",
    "    pred_scores = outputs[0]['scores'].data.cpu().numpy()\n",
    "    pred_labels = outputs[0]['labels'].data.cpu().numpy()\n",
    "\n",
    "    # Adding predictions to the list\n",
    "    for i, (box, score, label) in enumerate(zip(pred_boxes, pred_scores, pred_labels)):\n",
    "        prediction = {\n",
    "            'image_id': img_id,\n",
    "            'category_id': int(label),\n",
    "            'bbox': box.tolist(),\n",
    "            'score': float(score)\n",
    "        }\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    # Drawing bounding boxes and class labels on the image and saving it iff SAVE is True\n",
    "    if SAVE:\n",
    "        boxes, classes, labels, indices = predict(input_tensor, model, CONFIDENCE_THRESHOLD)\n",
    "        image = draw_boxes(boxes, labels, classes, image)\n",
    "        Image.fromarray(image).save(output_dir + sample)\n",
    "\n",
    "# Saving the predictions in a JSON file\n",
    "with open(output_dir + 'predictions.json', 'w') as f:\n",
    "    json.dump(predictions, f)\n",
    "\n",
    "# Computing mAP\n",
    "coco_dt = coco_gt.loadRes(output_dir + 'predictions.json')\n",
    "coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
    "coco_eval.evaluate()\n",
    "coco_eval.accumulate()\n",
    "coco_eval.summarize()\n",
    "\n",
    "mAP = coco_eval.stats[0]\n",
    "print(\"mAP: \", mAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load COCO annotations for the entire dataset\n",
    "coco = COCO('dataset/instances_val2017.json')\n",
    "\n",
    "# Load the pre-trained Faster R-CNN model trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "model.eval()\n",
    "\n",
    "# Initializing variables\n",
    "total_iou = 0.0\n",
    "total_boxes = 0\n",
    "\n",
    "# Iterating over all images in the dataset\n",
    "for img_id in coco.getImgIds():\n",
    "    image = read_image(DATASET_DIR, coco.loadImgs(img_id)[0]['file_name'])\n",
    "    input_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "    # Getting ground truth boxes corresponding to the image id\n",
    "    ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "    anns = coco.loadAnns(ann_ids)\n",
    "    gt_boxes = []\n",
    "    for ann in anns:\n",
    "        gt_boxes.append(ann['bbox'])\n",
    "    gt_boxes = np.array(gt_boxes)\n",
    "\n",
    "    # Getting predicted boxes\n",
    "    boxes, classes, labels, indices, scores = predict_with_scores(input_tensor, model, CONFIDENCE_THRESHOLD)\n",
    "    keep_indices = [i for i, score in enumerate(scores) if score >= CONFIDENCE_THRESHOLD]\n",
    "\n",
    "    # only keep boxes with score >= CONFIDENCE_THRESHOLD\n",
    "    boxes = [boxes[i] for i in keep_indices]\n",
    "\n",
    "    # Computing IoU\n",
    "    iou = 0.0\n",
    "    for gt_box in gt_boxes:\n",
    "        # Find box with highest IoU\n",
    "        max_iou = 0.0\n",
    "        for box in boxes:\n",
    "            iou = torchvision.ops.box_iou(torch.Tensor(box).unsqueeze(0), torch.Tensor(gt_box).unsqueeze(0))\n",
    "            if iou > max_iou:\n",
    "                max_iou = iou\n",
    "        total_iou += max_iou\n",
    "        total_boxes += 1\n",
    "\n",
    "print(\"Average IoU: \", total_iou.item() / total_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation visualization is a technique used in deep learning to understand and analyze the behavior of neural networks. It involves visualizing the activations or outputs of individual neurons or layers within a neural network when given a particular input.\n",
    "\n",
    "The main purpose of activation visualization is to gain insights into how the network is processing and representing the input data. By visualizing the activations, we can identify which parts of the input are being emphasized or ignored by the network. This can help us understand the network's decision-making process and potentially identify any issues or biases in the model.\n",
    "\n",
    "In general, the process involves the following steps:\n",
    "\n",
    "1. Load the pre-trained model: Activation visualization is typically performed on pre-trained models. The first step is to load the model architecture and weights.\n",
    "\n",
    "2. Prepare the input data: Depending on the task, you need to prepare the input data that the model expects. This could involve preprocessing, resizing, or normalizing the input images.\n",
    "\n",
    "3. Forward pass: Pass the input data through the model to obtain the activations. This involves feeding the input data through the layers of the model and collecting the activations at the desired layer(s).\n",
    "\n",
    "4. Visualize the activations: Once you have obtained the activations, you can visualize them using various techniques such as heatmaps, feature maps, or activation histograms. These visualizations can provide insights into the learned representations and patterns within the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_visualization(images, model, transform, show=False, save=True, split='val'):\n",
    "    output_dir = \"output/activation_visualization/\" + split + \"/\"\n",
    "    global first_layer_activations\n",
    "    global last_layer_activations\n",
    "    \n",
    "    for sample in images:\n",
    "\n",
    "        # Read the image from disk using the image_name\n",
    "        image = read_image(DATASET_DIR, sample)\n",
    "        image_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        output = model(image_tensor)\n",
    "        \n",
    "        first_layer_activations = first_layer_activations.squeeze(0).detach().cpu().numpy()\n",
    "        last_layer_activations = last_layer_activations.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "        # Nomralize each of the activations\n",
    "        for i in range(first_layer_activations.shape[0]):\n",
    "            first_layer_activations[i] = (first_layer_activations[i] - first_layer_activations[i].min()) / (first_layer_activations[i].max() - first_layer_activations[i].min()) * 255\n",
    "\n",
    "        for i in range(last_layer_activations.shape[0]):\n",
    "            last_layer_activations[i] = (last_layer_activations[i] - last_layer_activations[i].min()) / (last_layer_activations[i].max() - last_layer_activations[i].min()) * 255\n",
    "           \n",
    "        # Visualize the activations\n",
    "        # create a figure with three rows and three columns, first row is the original image (only one image)\n",
    "        fig, axes = plt.subplots(3, 3, figsize=(10, 5))\n",
    "\n",
    "        # Original image\n",
    "        for i in range(0, 3):\n",
    "            axes[0, i].axis('off')\n",
    "        axes[0, 1].imshow(image)\n",
    "        axes[0, 1].set_title(\"Original Image\")\n",
    "\n",
    "        # First layer activations for only the first 3 filters\n",
    "        axes[1, 0].imshow(torchvision.transforms.ToPILImage()(first_layer_activations[0]), cmap=\"gray\")\n",
    "        axes[1, 0].set_title(\"First Layer Activations\\nFilter 1\")\n",
    "        axes[1, 0].axis(\"off\")\n",
    "\n",
    "        axes[1, 1].imshow(torchvision.transforms.ToPILImage()(first_layer_activations[1]), cmap=\"gray\")\n",
    "        axes[1, 1].set_title(\"First Layer Activations\\nFilter 2\")\n",
    "        axes[1, 1].axis(\"off\")\n",
    "\n",
    "        axes[1, 2].imshow(torchvision.transforms.ToPILImage()(first_layer_activations[2]), cmap=\"gray\")\n",
    "        axes[1, 2].set_title(\"First Layer Activations\\nFilter 3\")\n",
    "        axes[1, 2].axis(\"off\")\n",
    "\n",
    "        # Last layer activations for only the first 3 filters\n",
    "        axes[2, 0].imshow(torchvision.transforms.ToPILImage()(last_layer_activations[0]), cmap=\"gray\")\n",
    "        axes[2, 0].set_title(\"Last Layer Activations\\nFilter 1\")\n",
    "        axes[2, 0].axis(\"off\")\n",
    "\n",
    "        axes[2, 1].imshow(torchvision.transforms.ToPILImage()(last_layer_activations[1]), cmap=\"gray\")\n",
    "        axes[2, 1].set_title(\"Last Layer Activations\\nFilter 2\")\n",
    "        axes[2, 1].axis(\"off\")\n",
    "\n",
    "        axes[2, 2].imshow(torchvision.transforms.ToPILImage()(last_layer_activations[2]), cmap=\"gray\")\n",
    "        axes[2, 2].set_title(\"Last Layer Activations\\nFilter 3\")\n",
    "        axes[2, 2].axis(\"off\")\n",
    "        plt.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.05, hspace=0.2, wspace=0.2)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            plt.savefig(output_dir + sample)\n",
    "\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "        first_layer_activations = None \n",
    "        last_layer_activations = None\n",
    "\n",
    "# Constants\n",
    "DATASET_DIR = \"dataset/val2017/\"\n",
    "print(\"Loaded {} labels from COCO dataset\".format(len(coco_labels)))\n",
    "\n",
    "# Load the pre-trained Faster R-CNN model trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Define the transformation to be applied to images\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "# Load the COCO dataset in \"dataset/test2017\"\n",
    "test_images = os.listdir(DATASET_DIR)\n",
    "\n",
    "# Specify the first and last layer of the model which will be used for visualization (hooked)\n",
    "first_layer = model.backbone.body.relu\n",
    "last_layer = model.backbone.body.layer4[2].conv3\n",
    "\n",
    "# Initializing activations which will be used in the hook functions\n",
    "first_layer_activations = None\n",
    "last_layer_activations = None\n",
    "\n",
    "# Defining hook for the first layer\n",
    "def first_layer_hook(module, input, output):\n",
    "    global first_layer_activations\n",
    "    first_layer_activations = output\n",
    "\n",
    "# Defining hook for the last layer\n",
    "def last_layer_hook(module, input, output):\n",
    "    global last_layer_activations\n",
    "    last_layer_activations = output\n",
    "\n",
    "# Registering the hooks\n",
    "first_layer.register_forward_hook(first_layer_hook)\n",
    "last_layer.register_forward_hook(last_layer_hook)\n",
    "\n",
    "# Apply activation visualization\n",
    "activation_visualization(test_images, model, transform, show=True, save=True, split='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"dataset/val2017/\"\n",
    "test_images = os.listdir(DATASET_DIR)\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "Gradcam = GradCAM(model,\n",
    "                    [model.backbone.body.layer4[2].conv3])\n",
    "\n",
    "for image in test_images:\n",
    "    image_name = image\n",
    "    image = np.array(Image.open(DATASET_DIR + image))\n",
    "    original_image = image.copy()\n",
    "    image_float_np = np.float32(image) / 255\n",
    "    \n",
    "    input_tensor = transform(image_float_np)\n",
    "    input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "    # Run the model and display the detections\n",
    "    boxes, classes, labels, indices = predict(input_tensor, model, 0.9)\n",
    "    targets = [FasterRCNNBoxScoreTarget(labels=labels, bounding_boxes=boxes)]\n",
    "\n",
    "    image_with_predictions = draw_boxes(boxes, labels, classes, image)\n",
    "\n",
    "    # Computing GradCAM\n",
    "    grayscale_gradcam = Gradcam(input_tensor=input_tensor, targets=targets)[0, :]\n",
    "    gradcam = show_cam_on_image(image_float_np, grayscale_gradcam, use_rgb=True)\n",
    "    if len(boxes) == 0:\n",
    "        continue\n",
    "    renormalized_gradcam = renormalize_cam_in_bounding_boxes(boxes, image_float_np, grayscale_gradcam)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(10, 5))\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title(\"Input Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    # Image with predicted bounding boxes\n",
    "    axes[1].imshow(image_with_predictions)\n",
    "    axes[1].set_title(\"Predicted Boxes\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    # GradCAM heatmap\n",
    "    axes[2].imshow(gradcam)\n",
    "    axes[2].set_title(\"GradCAM Heatmap\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    # GradCAM heatmap renormalized in bounding boxes\n",
    "    axes[3].imshow(renormalized_gradcam)\n",
    "    axes[3].set_title(\"GradCAM Heatmap\\nRenormalized in Bounding Boxes\")\n",
    "    axes[3].axis(\"off\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # Saving the images\n",
    "    output_dir = \"output/gradcam/val/\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    plt.savefig(output_dir + image_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradCAM++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"dataset/val2017/\"\n",
    "test_images = os.listdir(DATASET_DIR)\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "Gradcampp = GradCAMPlusPlus(model,\n",
    "                    [model.backbone.body.layer4[2].conv3])\n",
    "\n",
    "for image in test_images:\n",
    "    image_name = image\n",
    "    image = np.array(Image.open(DATASET_DIR + image))\n",
    "    original_image = image.copy()\n",
    "    image_float_np = np.float32(image) / 255\n",
    "    \n",
    "    input_tensor = transform(image_float_np)\n",
    "    input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "    # Run the model and display the detections\n",
    "    boxes, classes, labels, indices = predict(input_tensor, model, 0.9)\n",
    "    if len(boxes) == 0:\n",
    "        continue\n",
    "    targets = [FasterRCNNBoxScoreTarget(labels=labels, bounding_boxes=boxes)]\n",
    "\n",
    "    image_with_predictions = draw_boxes(boxes, labels, classes, image)\n",
    "\n",
    "    # Computing GradCAM\n",
    "    grayscale_gradcam = Gradcampp(input_tensor=input_tensor, targets=targets)[0, :]\n",
    "    gradcam = show_cam_on_image(image_float_np, grayscale_gradcam, use_rgb=True)\n",
    "    if len(boxes) == 0:\n",
    "        continue\n",
    "    renormalized_gradcam = renormalize_cam_in_bounding_boxes(boxes, image_float_np, grayscale_gradcam)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(10, 5))\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title(\"Input Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    # Image with predicted bounding boxes\n",
    "    axes[1].imshow(image_with_predictions)\n",
    "    axes[1].set_title(\"Predicted Boxes\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    # GradCAM heatmap\n",
    "    axes[2].imshow(gradcam)\n",
    "    axes[2].set_title(\"GradCAM++ Heatmap\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    # GradCAM heatmap renormalized in bounding boxes\n",
    "    axes[3].imshow(renormalized_gradcam)\n",
    "    axes[3].set_title(\"GradCAM++ Heatmap\\nRenormalized in Bounding Boxes\")\n",
    "    axes[3].axis(\"off\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # Saving the images\n",
    "    output_dir = \"output/gradcampp/val/\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    plt.savefig(output_dir + image_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EigenCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"dataset/val2017/\"\n",
    "test_images = os.listdir(DATASET_DIR)\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "target_layers = [model.backbone]\n",
    "\n",
    "Eigencam = EigenCAM(model,\n",
    "               target_layers, \n",
    "               reshape_transform=fasterrcnn_reshape_transform)\n",
    "\n",
    "for image in test_images:\n",
    "    image_name = image\n",
    "    image = np.array(Image.open(DATASET_DIR + image))\n",
    "    original_image = image.copy()\n",
    "    image_float_np = np.float32(image) / 255\n",
    "    \n",
    "    input_tensor = transform(image_float_np)\n",
    "    input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "    # Run the model and display the detections\n",
    "    boxes, classes, labels, indices = predict(input_tensor, model, 0.9)\n",
    "    targets = [FasterRCNNBoxScoreTarget(labels=labels, bounding_boxes=boxes)]\n",
    "\n",
    "    image_with_predictions = draw_boxes(boxes, labels, classes, image)\n",
    "\n",
    "    # Computing EigenCAM\n",
    "    grayscale_eigencam = Eigencam(input_tensor=input_tensor, targets=targets)[0, :]\n",
    "    eigencam_image = show_cam_on_image(image_float_np, grayscale_eigencam, use_rgb=True)\n",
    "    if len(boxes) == 0:\n",
    "        continue\n",
    "    renormalized_eigencam_image = renormalize_cam_in_bounding_boxes(boxes, image_float_np, grayscale_eigencam)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(10, 5))\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title(\"Input Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    # Image with predicted bounding boxes\n",
    "    axes[1].imshow(image_with_predictions)\n",
    "    axes[1].set_title(\"Predicted Boxes\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    # EigenCAM heatmap\n",
    "    axes[2].imshow(eigencam_image)\n",
    "    axes[2].set_title(\"EigenCAM Heatmap\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    # EigenCAM heatmap renormalized in bounding boxes\n",
    "    axes[3].imshow(renormalized_eigencam_image)\n",
    "    axes[3].set_title(\"EigenCAM Heatmap\\nRenormalized in Bounding Boxes\")\n",
    "    axes[3].axis(\"off\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # Saving the images\n",
    "    output_dir = \"output/eigencam/val/\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    plt.savefig(output_dir + image_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AblationCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"dataset/val2017/\"\n",
    "test_images = os.listdir(DATASET_DIR)\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "target_layers = [model.backbone]\n",
    "\n",
    "Ablationcam = AblationCAM(model,\n",
    "                  target_layers, \n",
    "                  reshape_transform=fasterrcnn_reshape_transform,\n",
    "                  ablation_layer=AblationLayerFasterRCNN(),\n",
    "                  ratio_channels_to_ablate=0.01)\n",
    "\n",
    "for image in test_images:\n",
    "    image_name = image\n",
    "    image = np.array(Image.open(DATASET_DIR + image))\n",
    "    original_image = image.copy()\n",
    "    image_float_np = np.float32(image) / 255\n",
    "    \n",
    "    input_tensor = transform(image_float_np)\n",
    "    input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "    # Run the model and display the detections\n",
    "    boxes, classes, labels, indices = predict(input_tensor, model, 0.9)\n",
    "    targets = [FasterRCNNBoxScoreTarget(labels=labels, bounding_boxes=boxes)]\n",
    "\n",
    "    image_with_predictions = draw_boxes(boxes, labels, classes, image)\n",
    "\n",
    "    # Computing AblationCAM\n",
    "    grayscale_ablationcam = Ablationcam(input_tensor=input_tensor, targets=targets)[0, :]\n",
    "    ablationcam = show_cam_on_image(image_float_np, grayscale_ablationcam, use_rgb=True)\n",
    "    if len(boxes) == 0:\n",
    "        continue\n",
    "    renormalized_ablationcam = renormalize_cam_in_bounding_boxes(boxes, image_float_np, grayscale_ablationcam)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(10, 5))\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title(\"Input Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    # Image with predicted bounding boxes\n",
    "    axes[1].imshow(image_with_predictions)\n",
    "    axes[1].set_title(\"Predicted Boxes\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    # AblationCAM heatmap\n",
    "    axes[2].imshow(ablationcam)\n",
    "    axes[2].set_title(\"AblationCAM Heatmap\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    # AblationCAM heatmap renormalized in bounding boxes\n",
    "    axes[3].imshow(renormalized_ablationcam)\n",
    "    axes[3].set_title(\"AblationCAM Heatmap\\nRenormalized in Bounding Boxes\")\n",
    "    axes[3].axis(\"off\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # Saving the images\n",
    "    output_dir = \"output/ablation/val/\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    plt.savefig(output_dir + image_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Feature Factorizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(model.roi_heads.box_roi_pool.output_size[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"dataset/val2017/\"\n",
    "test_images = os.listdir(DATASET_DIR)\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "dff = DeepFeatureFactorization(model=model, target_layer=model.backbone.body.layer3[5].conv3, computation_on_concepts=model.roi_heads.box_predictor.cls_score)\n",
    "\n",
    "\n",
    "for image in test_images:\n",
    "    image_name = image\n",
    "    image = np.array(Image.open(DATASET_DIR + image))\n",
    "    original_image = image.copy()\n",
    "    image_float_np = np.float32(image) / 255\n",
    "    \n",
    "    input_tensor = transform(image_float_np)\n",
    "    input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "    # Run the model and display the detections\n",
    "    boxes, classes, labels, indices = predict(input_tensor, model, 0.9)\n",
    "\n",
    "    image_with_predictions = draw_boxes(boxes, labels, classes, image)\n",
    "    \n",
    "    # Computing Deep Feature Factorization\n",
    "    concepts_2, batch_explanations_2, concept_scores_2 = dff(input_tensor, n_components = 2)\n",
    "    visualization_2 = show_factorization_on_image(image_float_np, \n",
    "                                                batch_explanations_2[0],\n",
    "                                                image_weight=0.3)\n",
    "    \n",
    "    # Computing again for n_components = 3\n",
    "    concepts_3, batch_explanations_3, concept_scores_3 = dff(input_tensor, n_components = 3)\n",
    "    visualization_3 = show_factorization_on_image(image_float_np, \n",
    "                                                batch_explanations_3[0],\n",
    "                                                image_weight=0.3)\n",
    "    \n",
    "    # Computing again for n_components = 5\n",
    "    concepts_5, batch_explanations_5, concept_scores_5 = dff(input_tensor, n_components = 5)\n",
    "    visualization_5 = show_factorization_on_image(image_float_np, \n",
    "                                                batch_explanations_5[0],\n",
    "                                                image_weight=0.3)\n",
    "        \n",
    "    if len(boxes) == 0:\n",
    "        continue\n",
    "\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(10, 5))\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title(\"Input Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "    \n",
    "    # Image with predicted bounding boxes\n",
    "    axes[1].imshow(image_with_predictions)\n",
    "    axes[1].set_title(\"Predicted Boxes\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    # Deep Feature Factorization heatmap\n",
    "    axes[2].imshow(visualization_2)\n",
    "    axes[2].set_title(\"DFF Heatmap (2)\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    # Deep Feature Factorization heatmap\n",
    "    axes[3].imshow(visualization_3)\n",
    "    axes[3].set_title(\"DFF Heatmap (3)\")\n",
    "    axes[3].axis(\"off\")\n",
    "\n",
    "    # Deep Feature Factorization heatmap\n",
    "    axes[4].imshow(visualization_5)\n",
    "    axes[4].set_title(\"DFF Heatmap (5)\")\n",
    "    axes[4].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # Saving the images\n",
    "    output_dir = \"output/dff/val/\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    plt.savefig(output_dir + image_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ScoreCAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So far it causes the computer to crash. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"dataset/val2017/\"\n",
    "test_images = os.listdir(DATASET_DIR)\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "target_layers = [model.backbone.body.layer4[2].conv3]\n",
    "\n",
    "Scorecam = ScoreCAM(model,\n",
    "                target_layers)\n",
    "\n",
    "for image in test_images:\n",
    "    image_name = image\n",
    "    image = np.array(Image.open(DATASET_DIR + image))\n",
    "    original_image = image.copy()\n",
    "    image_float_np = np.float32(image) / 255\n",
    "    \n",
    "    input_tensor = transform(image_float_np)\n",
    "    input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "    # Run the model and display the detections\n",
    "    boxes, classes, labels, indices = predict(input_tensor, model, 0.9)\n",
    "    targets = [FasterRCNNBoxScoreTarget(labels=labels, bounding_boxes=boxes)]\n",
    "\n",
    "    image_with_predictions = draw_boxes(boxes, labels, classes, image)\n",
    "\n",
    "    # Computing ScoreCAM\n",
    "    grayscale_scorecam = Scorecam(input_tensor=input_tensor, targets=targets)[0, :]\n",
    "    scorecam = show_cam_on_image(image_float_np, grayscale_scorecam, use_rgb=True)\n",
    "    if len(boxes) == 0:\n",
    "        continue\n",
    "    renormalized_scorecam = renormalize_cam_in_bounding_boxes(boxes, image_float_np, grayscale_scorecam)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(10, 5))\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title(\"Input Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    # Image with predicted bounding boxes\n",
    "    axes[1].imshow(image_with_predictions)\n",
    "    axes[1].set_title(\"Predicted Boxes\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    # ScoreCAM heatmap\n",
    "    axes[2].imshow(scorecam)\n",
    "    axes[2].set_title(\"ScoreCAM Heatmap\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    # ScoreCAM heatmap renormalized in bounding boxes\n",
    "    axes[3].imshow(renormalized_scorecam)\n",
    "    axes[3].set_title(\"ScoreCAM Heatmap\\nRenormalized in Bounding Boxes\")\n",
    "    axes[3].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # Saving the images\n",
    "    output_dir = \"output/scorecam/val/\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    plt.savefig(output_dir + image_name)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
